{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdORwL8ta4ve",
        "outputId": "babd5012-1b95-4954-894c-c0c329a2865a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unsloth\n",
            "  Downloading unsloth-2026.2.1-py3-none-any.whl.metadata (69 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/69.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.49.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.34-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.1)\n",
            "Collecting trl\n",
            "  Downloading trl-0.28.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.22.2)\n",
            "Downloading unsloth-2026.2.1-py3-none-any.whl (432 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.3/432.3 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.49.2-py3-none-manylinux_2_24_x86_64.whl (60.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.34-cp39-abi3-manylinux_2_28_x86_64.whl (110.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.28.0-py3-none-any.whl (540 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.5/540.5 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xformers, unsloth, trl, bitsandbytes\n",
            "Successfully installed bitsandbytes-0.49.2 trl-0.28.0 unsloth-2026.2.1 xformers-0.0.34\n",
            "Requirement already satisfied: transformers>=4.51.0 in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (5.29.6)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (1.4.0)\n",
            "Collecting hf_transfer\n",
            "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.5.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.3)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub) (0.16.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface_hub) (8.3.1)\n",
            "Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hf_transfer\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth 2026.2.1 requires tyro, which is not installed.\n",
            "unsloth 2026.2.1 requires unsloth_zoo>=2026.2.1, which is not installed.\n",
            "unsloth 2026.2.1 requires datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1, but you have datasets 4.0.0 which is incompatible.\n",
            "unsloth 2026.2.1 requires transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,!=4.57.4,!=4.57.5,<=4.57.6,>=4.51.3, but you have transformers 5.0.0 which is incompatible.\n",
            "unsloth 2026.2.1 requires trl!=0.19.0,<=0.24.0,>=0.18.2, but you have trl 0.28.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed hf_transfer-0.1.9\n",
            "Collecting unsloth_zoo\n",
            "  Downloading unsloth_zoo-2026.2.1-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo) (2.9.0+cu128)\n",
            "Collecting torchao>=0.13.0 (from unsloth_zoo)\n",
            "  Downloading torchao-0.16.0-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo) (3.5.0)\n",
            "Requirement already satisfied: packaging>=24.1 in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo) (26.0)\n",
            "Collecting tyro (from unsloth_zoo)\n",
            "  Downloading tyro-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.4,!=4.57.5,<=4.57.6,>=4.51.3 (from unsloth_zoo)\n",
            "  Downloading transformers-4.57.6-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1 (from unsloth_zoo)\n",
            "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo) (0.2.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo) (4.67.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo) (0.46.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo) (2.0.2)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo) (1.12.0)\n",
            "Collecting trl!=0.19.0,<=0.24.0,>=0.18.2 (from unsloth_zoo)\n",
            "  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo) (0.18.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo) (5.29.6)\n",
            "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo) (1.4.0)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo) (0.1.9)\n",
            "Collecting cut_cross_entropy (from unsloth_zoo)\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo) (11.3.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo) (2025.11.3)\n",
            "Collecting msgspec (from unsloth_zoo)\n",
            "  Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth_zoo) (6.0.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth_zoo) (0.7.0)\n",
            "Collecting pyarrow>=21.0.0 (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo)\n",
            "  Downloading pyarrow-23.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (2.32.4)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth_zoo) (1.2.0)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth_zoo) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth_zoo) (0.21.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth_zoo) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth_zoo) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth_zoo) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth_zoo) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth_zoo) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth_zoo) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth_zoo) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth_zoo) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth_zoo) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth_zoo) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth_zoo) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth_zoo) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth_zoo) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth_zoo) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth_zoo) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth_zoo) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth_zoo) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth_zoo) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth_zoo) (1.13.1.3)\n",
            "Collecting huggingface_hub>=0.34.0 (from unsloth_zoo)\n",
            "  Downloading huggingface_hub-0.36.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.4,!=4.57.5,<=4.57.6,>=4.51.3->unsloth_zoo) (0.22.2)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth_zoo) (0.17.0)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth_zoo) (4.4.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (3.13.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (2.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth_zoo) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth_zoo) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth_zoo) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface_hub>=0.34.0->unsloth_zoo) (8.3.1)\n",
            "Downloading unsloth_zoo-2026.2.1-py3-none-any.whl (376 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.5/376.5 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchao-0.16.0-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m121.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.57.6-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.36.2-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.24.0-py3-none-any.whl (423 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-1.0.6-py3-none-any.whl (181 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-23.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (47.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchao, pyarrow, msgspec, tyro, huggingface_hub, transformers, datasets, cut_cross_entropy, trl, unsloth_zoo\n",
            "  Attempting uninstall: torchao\n",
            "    Found existing installation: torchao 0.10.0\n",
            "    Uninstalling torchao-0.10.0:\n",
            "      Successfully uninstalled torchao-0.10.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface_hub 1.4.0\n",
            "    Uninstalling huggingface_hub-1.4.0:\n",
            "      Successfully uninstalled huggingface_hub-1.4.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 5.0.0\n",
            "    Uninstalling transformers-5.0.0:\n",
            "      Successfully uninstalled transformers-5.0.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "  Attempting uninstall: trl\n",
            "    Found existing installation: trl 0.28.0\n",
            "    Uninstalling trl-0.28.0:\n",
            "      Successfully uninstalled trl-0.28.0\n",
            "Successfully installed cut_cross_entropy-25.1.1 datasets-4.3.0 huggingface_hub-0.36.2 msgspec-0.20.0 pyarrow-23.0.1 torchao-0.16.0 transformers-4.57.6 trl-0.24.0 tyro-1.0.6 unsloth_zoo-2026.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --no-deps unsloth bitsandbytes accelerate xformers peft trl tokenizers\n",
        "!pip install --no-deps \"transformers>=4.51.0\"\n",
        "!pip install sentencepiece protobuf huggingface_hub hf_transfer\n",
        "!pip install unsloth_zoo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zero Shot Inference on base models\n",
        "\n",
        "Environment Setup: Mounts Google Drive and defines paths for datasets and inference results.\n",
        "\n",
        "Model Loading: load base model: Gemma, Qwen, Llama\n"
      ],
      "metadata": {
        "id": "dY8Igyc4x7D3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "\n",
        "#  MODEL SELECTION\n",
        "# MODEL_ID = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
        "# MODEL_ID = \"unsloth/gemma-2-2b-it-bnb-4bit\"\n",
        "MODEL_ID = \"Qwen/Qwen3-0.6B\"\n",
        "\n",
        "\n",
        "# Path\n",
        "BASE_DIR = \"/content/drive/MyDrive/DnD_Project_Data\"\n",
        "TEST_DATA_PATH = os.path.join(BASE_DIR, \"processed_dataset\", \"dnd_test.json\")\n",
        "BASE_RESULTS_DIR = os.path.join(BASE_DIR, \"inference_results_baseline\")\n",
        "\n",
        "safe_model_name = MODEL_ID.split(\"/\")[-1].replace(\"-bnb-4bit\", \"\")\n",
        "MODEL_SPECIFIC_DIR = os.path.join(BASE_RESULTS_DIR, safe_model_name)\n",
        "\n",
        "# Create directory\n",
        "os.makedirs(MODEL_SPECIFIC_DIR, exist_ok=True)\n",
        "\n",
        "# Prompt Template\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# MEMORY CLEANUP\n",
        "\n",
        "print(f\"Cleaning GPU memory before loading {safe_model_name}...\")\n",
        "\n",
        "# Delete previous instances if they exist\n",
        "if 'model' in globals():\n",
        "    del model\n",
        "if 'tokenizer' in globals():\n",
        "    del tokenizer\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"[INFO] Memory cleaned.\")\n",
        "\n",
        "\n",
        "# LOAD BASE MODEL\n",
        "\n",
        "print(f\"[INFO] Loading Model: {MODEL_ID}...\")\n",
        "\n",
        "try:\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = MODEL_ID,\n",
        "        max_seq_length = 2048,\n",
        "        dtype = None,\n",
        "        load_in_4bit = True,\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load model {MODEL_ID}. Error: {e}\")\n",
        "    raise e\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Tokenizer Configuration\n",
        "tokenizer.padding_side = \"left\"\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "#Inference\n",
        "output_file = os.path.join(MODEL_SPECIFIC_DIR, f\"predictions_{safe_model_name}_BASELINE.json\")\n",
        "print(f\"[INFO] Output file set to: {output_file}\")\n",
        "\n",
        "# Load Test Dataset\n",
        "with open(TEST_DATA_PATH, 'r') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "results = []\n",
        "\n",
        "# Resume Capability\n",
        "if os.path.exists(output_file):\n",
        "    try:\n",
        "        with open(output_file, 'r') as f:\n",
        "            results = json.load(f)\n",
        "        print(f\"Resuming from example {len(results)}\")\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Output file corrupted or empty. Starting from scratch.\")\n",
        "        results = []\n",
        "\n",
        "start_index = len(results)\n",
        "data_slice = test_data[start_index:]\n",
        "\n",
        "# Generation Parameters\n",
        "gen_kwargs = {\n",
        "    \"max_new_tokens\": 1024,\n",
        "    \"use_cache\": True,\n",
        "    \"temperature\": 0.2,\n",
        "    \"min_p\": 0.1,\n",
        "    \"pad_token_id\": tokenizer.pad_token_id,\n",
        "    \"eos_token_id\": tokenizer.eos_token_id\n",
        "}\n",
        "\n",
        "# Main Loop\n",
        "if len(data_slice) > 0:\n",
        "    for i in tqdm(range(0, len(data_slice), BATCH_SIZE), desc=f\"Testing {safe_model_name}\"):\n",
        "\n",
        "        # Prepare Batch\n",
        "        batch_samples = data_slice[i : i + BATCH_SIZE]\n",
        "        batch_prompts = []\n",
        "\n",
        "        for sample in batch_samples:\n",
        "            raw_prompt = sample['llm_prompt']\n",
        "\n",
        "            # Parse Instruction vs Input\n",
        "            if \"Input Data:\" in raw_prompt:\n",
        "                parts = raw_prompt.split(\"Input Data:\")\n",
        "                instr = parts[0].strip()\n",
        "                inp_data = parts[1].strip()\n",
        "            elif \"Character Sheet:\" in raw_prompt:\n",
        "                parts = raw_prompt.split(\"Character Sheet:\")\n",
        "                instr = parts[0].strip()\n",
        "                inp_data = parts[1].strip()\n",
        "            else:\n",
        "                instr = raw_prompt\n",
        "                inp_data = \"\"\n",
        "\n",
        "            # Format using Alpaca Template\n",
        "            full_prompt = alpaca_prompt.format(instr, inp_data, \"\")\n",
        "            batch_prompts.append(full_prompt)\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = tokenizer(\n",
        "            batch_prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, **gen_kwargs)\n",
        "\n",
        "        # Decode\n",
        "        input_len = inputs.input_ids.shape[1]\n",
        "        generated_tokens = outputs[:, input_len:]\n",
        "        decoded_responses = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "        # Results\n",
        "        for idx, resp in enumerate(decoded_responses):\n",
        "            results.append({\n",
        "                \"task_type\": batch_samples[idx].get(\"task_type\"),\n",
        "                \"input_prompt\": batch_prompts[idx],\n",
        "                \"generated_response\": resp.strip(),\n",
        "                \"expected_output\": batch_samples[idx].get(\"expected_output\")\n",
        "            })\n",
        "\n",
        "        # Save Checkpoint\n",
        "        with open(output_file, 'w') as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "\n",
        "    print(f\"Baseline inference completed for {safe_model_name}.\")\n",
        "else:\n",
        "    print(f\"Inference already completed for {safe_model_name}.\")"
      ],
      "metadata": {
        "id": "MGcEFFjPyBqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-Tuning Pipeline\n",
        "\n",
        "Environment Setup: Mounts Google Drive and defines paths for datasets, model storage, and inference results.\n",
        "\n",
        "Model Loading: Supports loading varying base models (e.g., Gemma 2, Qwen, Llama 3) in 4-bit quantization (for GPU limits).\n",
        "\n",
        "LoRA Configuration: Applies Low-Rank Adaptation (LoRA) to efficiently fine-tune the model parameters\n",
        "\n",
        "Training Loop:\n",
        "* Checks if a fine-tuned adapter already exists\n",
        "* If not, initiates the trainer on the processed training dataset\n",
        "* Uses the Alpaca prompt format (Instruction/Input/Response)\n",
        "\n",
        "Batch Inference: Runs optimized inference on the test set\n"
      ],
      "metadata": {
        "id": "uh0UICoE3h3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive, userdata\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# PATH\n",
        "BASE_DIR = \"/content/drive/MyDrive/DnD_Project_Data\"\n",
        "TRAIN_DATA_PATH = os.path.join(BASE_DIR, \"processed_dataset\", \"dnd_train.json\")\n",
        "TEST_DATA_PATH = os.path.join(BASE_DIR, \"processed_dataset\", \"dnd_test.json\")\n",
        "OUTPUT_MODELS_DIR = os.path.join(BASE_DIR, \"fine_tuned_models_unsloth\")\n",
        "OUTPUT_RESULTS_DIR = os.path.join(BASE_DIR, \"inference_results\")\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(OUTPUT_MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "# MODEL SELECTION\n",
        "MODELS_TO_TRAIN = [\n",
        "    \"Qwen/Qwen3-0.6B\",\n",
        "    #\"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
        "    #\"unsloth/gemma-2-2b-it-bnb-4bit\"\n",
        "]\n",
        "\n",
        "# Training Configuration\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "DTYPE = None\n",
        "LOAD_IN_4BIT = True\n",
        "\n",
        "# Prompt Template\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# UTILITY FUNCTIONS\n",
        "\n",
        "\n",
        "def clean_memory():\n",
        "    import gc\n",
        "    import torch\n",
        "    print(\"Cleaning memory\")\n",
        "\n",
        "    target_vars = ['model', 'tokenizer', 'trainer']\n",
        "\n",
        "    for var_name in target_vars:\n",
        "        if var_name in globals():\n",
        "            del globals()[var_name]\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Memoria pulita.\\n\")\n",
        "\n",
        "def format_prompts(examples):\n",
        "    \"\"\"Formats the raw dataset examples into the Alpaca prompt structure.\"\"\"\n",
        "    EOS_TOKEN = tokenizer.eos_token\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "\n",
        "    for instruction, input_data, output_data in zip(instructions, inputs, outputs):\n",
        "        # Convert dict/list inputs to JSON string for consistency\n",
        "        input_str = json.dumps(input_data, indent=2) if isinstance(input_data, (dict, list)) else str(input_data)\n",
        "        output_str = json.dumps(output_data, indent=2) if isinstance(output_data, (dict, list)) else str(output_data)\n",
        "\n",
        "        # Format text\n",
        "        text = alpaca_prompt.format(instruction, input_str, output_str) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "\n",
        "    return { \"text\" : texts }\n",
        "\n",
        "def run_full_inference_benchmark(model, tokenizer, test_path, model_name):\n",
        "    \"\"\"\n",
        "    Batch Inference with periodic saving to Drive\n",
        "    \"\"\"\n",
        "    print(f\"\\nSTARTING FULL INFERENCE (BATCH MODE) FOR {model_name}...\")\n",
        "\n",
        "    # 1. SETUP\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    tokenizer.padding_side = \"left\"\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    BATCH_SIZE = 16\n",
        "\n",
        "    with open(test_path, 'r') as f:\n",
        "        test_data = json.load(f)\n",
        "\n",
        "    safe_name = model_name.split(\"/\")[-1].replace(\"-bnb-4bit\", \"\")\n",
        "    output_file = os.path.join(OUTPUT_RESULTS_DIR, f\"predictions_{safe_name}.json\")\n",
        "\n",
        "    # RESUME LOGIC\n",
        "    results = []\n",
        "    start_index = 0\n",
        "\n",
        "    if os.path.exists(output_file):\n",
        "        print(f\"Found partial results file: {output_file}\")\n",
        "        try:\n",
        "            with open(output_file, 'r') as f:\n",
        "                results = json.load(f)\n",
        "            start_index = len(results)\n",
        "            print(f\"Resuming from example {start_index} of {len(test_data)}\")\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Partial file corrupted. Restarting from zero.\")\n",
        "            results = []\n",
        "\n",
        "    if start_index >= len(test_data):\n",
        "        print(\"Inference already completed for this model!\")\n",
        "        return\n",
        "\n",
        "    # BATCH LOOP\n",
        "    data_slice = test_data[start_index:]\n",
        "\n",
        "    # Configure generation parameters based on model type\n",
        "    gen_kwargs = {\n",
        "        \"max_new_tokens\": 1024,\n",
        "        \"use_cache\": True,\n",
        "        \"pad_token_id\": tokenizer.pad_token_id,\n",
        "        \"eos_token_id\": tokenizer.eos_token_id,\n",
        "        \"temperature\": 0.2, # Low temperature for more deterministic results\n",
        "        \"min_p\": 0.1\n",
        "    }\n",
        "\n",
        "    if \"qwen\" in model_name.lower():\n",
        "        gen_kwargs[\"repetition_penalty\"] = 1.1\n",
        "\n",
        "    # Process data in chunks\n",
        "    for i in tqdm(range(0, len(data_slice), BATCH_SIZE), desc=f\"Testing {safe_name}\"):\n",
        "\n",
        "        # Prepare Batch\n",
        "        batch_samples = data_slice[i : i + BATCH_SIZE]\n",
        "        batch_prompts = []\n",
        "\n",
        "        for sample in batch_samples:\n",
        "            raw_prompt = sample['llm_prompt']\n",
        "\n",
        "            # Simple parsing to separate Instruction from Input for formatting\n",
        "            if \"Input Data:\" in raw_prompt:\n",
        "                parts = raw_prompt.split(\"Input Data:\")\n",
        "                instr = parts[0].strip()\n",
        "                inp_data = parts[1].strip()\n",
        "            elif \"Character Sheet:\" in raw_prompt:\n",
        "                parts = raw_prompt.split(\"Character Sheet:\")\n",
        "                instr = parts[0].strip()\n",
        "                inp_data = parts[1].strip()\n",
        "            else:\n",
        "                instr = raw_prompt\n",
        "                inp_data = \"\"\n",
        "\n",
        "            full_prompt = alpaca_prompt.format(instr, inp_data, \"\")\n",
        "            batch_prompts.append(full_prompt)\n",
        "\n",
        "        # Tokenize Batch\n",
        "        inputs = tokenizer(\n",
        "            batch_prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                **gen_kwargs\n",
        "            )\n",
        "\n",
        "        # Decode\n",
        "        input_len = inputs.input_ids.shape[1]\n",
        "        generated_tokens = outputs[:, input_len:]\n",
        "        decoded_responses = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "        # Store\n",
        "        for idx, resp in enumerate(decoded_responses):\n",
        "            original_sample = batch_samples[idx]\n",
        "            results.append({\n",
        "                \"task_type\": original_sample.get(\"task_type\"),\n",
        "                \"input_prompt\": batch_prompts[idx],\n",
        "                \"generated_response\": resp.strip(),\n",
        "                \"expected_output\": original_sample.get(\"expected_output\")\n",
        "            })\n",
        "\n",
        "        # SAVE CHECKPOINT\n",
        "        with open(output_file, 'w') as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "            f.flush()\n",
        "            os.fsync(f.fileno())\n",
        "\n",
        "    print(f\"Results saved to: {output_file}\")\n",
        "\n",
        "\n",
        "# MAIN PIPELINE - TRAIN & INFERENCE\n",
        "\n",
        "\n",
        "for model_name in MODELS_TO_TRAIN:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"PIPELINE STARTED FOR: {model_name}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    safe_name = model_name.split(\"/\")[-1].replace(\"-bnb-4bit\", \"\")\n",
        "    save_path = os.path.join(OUTPUT_MODELS_DIR, f\"{safe_name}_DnD_Adapter\")\n",
        "\n",
        "    try:\n",
        "        # CHECK IF MODEL EXISTS\n",
        "        if os.path.exists(save_path):\n",
        "            print(f\"FOUND EXISTING ADAPTER AT: {save_path}\")\n",
        "            print(\"Skipping training phase. Loading adapter from Drive\")\n",
        "\n",
        "            model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "                model_name = save_path,\n",
        "                max_seq_length = MAX_SEQ_LENGTH,\n",
        "                dtype = DTYPE,\n",
        "                load_in_4bit = LOAD_IN_4BIT,\n",
        "            )\n",
        "            should_train = False\n",
        "\n",
        "        else:\n",
        "            print(f\"No existing adapter found, starting training \")\n",
        "\n",
        "            # Load base model\n",
        "            model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "                model_name = model_name,\n",
        "                max_seq_length = MAX_SEQ_LENGTH,\n",
        "                dtype = DTYPE,\n",
        "                load_in_4bit = LOAD_IN_4BIT,\n",
        "            )\n",
        "\n",
        "            # Configure LoRA\n",
        "            model = FastLanguageModel.get_peft_model(\n",
        "                model,\n",
        "                r = 16,\n",
        "                target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                                  \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                lora_alpha = 16,\n",
        "                lora_dropout = 0,\n",
        "                bias = \"none\",\n",
        "                use_gradient_checkpointing = \"unsloth\",\n",
        "                random_state = 3407,\n",
        "                use_rslora = False,\n",
        "                loftq_config = None,\n",
        "            )\n",
        "            should_train = True\n",
        "\n",
        "        # TRAINING\n",
        "        if should_train:\n",
        "            dataset = load_dataset(\"json\", data_files=TRAIN_DATA_PATH, split=\"train\")\n",
        "            dataset = dataset.map(format_prompts, batched = True)\n",
        "\n",
        "            os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "            print(\"Starting SFT Training...\")\n",
        "            trainer = SFTTrainer(\n",
        "                model = model,\n",
        "                tokenizer = tokenizer,\n",
        "                train_dataset = dataset,\n",
        "                dataset_text_field = \"text\",\n",
        "                max_seq_length = MAX_SEQ_LENGTH,\n",
        "                dataset_num_proc = 2,\n",
        "                packing = False,\n",
        "                args = TrainingArguments(\n",
        "                    per_device_train_batch_size = 2,\n",
        "                    gradient_accumulation_steps = 4,\n",
        "                    warmup_steps = 10,\n",
        "                    num_train_epochs = 1,\n",
        "                    learning_rate = 2e-4,\n",
        "                    fp16 = not is_bfloat16_supported(),\n",
        "                    bf16 = is_bfloat16_supported(),\n",
        "                    logging_steps = 50,\n",
        "                    optim = \"adamw_8bit\",\n",
        "                    weight_decay = 0.01,\n",
        "                    lr_scheduler_type = \"linear\",\n",
        "                    seed = 3407,\n",
        "                    output_dir = \"outputs\",\n",
        "                    report_to = \"none\",\n",
        "                ),\n",
        "            )\n",
        "            trainer.train()\n",
        "\n",
        "            print(f\"Saving Adapter to: {save_path}\")\n",
        "            model.save_pretrained(save_path)\n",
        "            tokenizer.save_pretrained(save_path)\n",
        "\n",
        "        # INFERENCE\n",
        "        run_full_inference_benchmark(model, tokenizer, TEST_DATA_PATH, model_name)\n",
        "\n",
        "        # CLEANUP\n",
        "        clean_memory()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"error with {model_name}: {e}\")\n",
        "        clean_memory()\n",
        "        continue"
      ],
      "metadata": {
        "id": "UwGop7v_a58F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ONE SHOT INFERENCE - on fine-tuned model"
      ],
      "metadata": {
        "id": "_7YfXnzn1Jqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import json\n",
        "import gc\n",
        "from unsloth import FastLanguageModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "# PATH\n",
        "BASE_DIR = \"/content/drive/MyDrive/DnD_Project_Data\"\n",
        "ADAPTER_PATH = os.path.join(BASE_DIR, \"fine_tuned_models_unsloth\", \"Llama-3.2-1B-Instruct_DnD_Adapter\")\n",
        "TEST_DATA_PATH = os.path.join(BASE_DIR, \"processed_dataset\", \"dnd_test.json\")\n",
        "OUTPUT_FILE = os.path.join(BASE_DIR, \"inference_results\", \"predictions_Llama_1B_OneShot_Smart.json\")\n",
        "\n",
        "# LOAD\n",
        "try:\n",
        "    del model\n",
        "    del tokenizer\n",
        "except:\n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"loading model: {ADAPTER_PATH}...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = ADAPTER_PATH,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# EXAMPLES\n",
        "alpaca_template = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{}\\n\\n### Input:\\n{}\\n\\n### Response:\\n{}\"\n",
        "\n",
        "# GENERATION\n",
        "ex_gen_instr = \"Generate a complete D&D 5e character sheet based on the provided attributes.\"\n",
        "ex_gen_input = '{\"race\": \"Hill Dwarf\", \"class\": \"Cleric\", \"level\": 1, \"background\": \"Acolyte\", \"stats\": {\"str\": 14, \"dex\": 8, \"con\": 15, \"int\": 10, \"wis\": 16, \"cha\": 12}}'\n",
        "ex_gen_output = \"\"\"```json\n",
        "{\n",
        "  \"race\": \"Hill Dwarf\",\n",
        "  \"subrace\": null,\n",
        "  \"class\": \"Cleric\",\n",
        "  \"subclass\": \"Life Domain\",\n",
        "  \"level\": 1,\n",
        "  \"background\": \"Acolyte\",\n",
        "  \"stats\": {\"str\": 14, \"dex\": 8, \"con\": 15, \"int\": 10, \"wis\": 16, \"cha\": 12},\n",
        "  \"hp\": 11,\n",
        "  \"ac\": 16,\n",
        "  \"alignment\": \"Lawful Good\",\n",
        "  \"skills\": [\"Insight\", \"Religion\", \"Medicine\"],\n",
        "  \"weapons\": [\"Warhammer\", \"Light Crossbow\"],\n",
        "  \"spells\": [\"Bless\", \"Cure Wounds\", \"Sacred Flame\"],\n",
        "  \"feats\": []\n",
        "}\n",
        "```\"\"\"\n",
        "\n",
        "# COMPLETION\n",
        "ex_fill_instr = \"Complete this D&D 5e character sheet. Replace the NULL value(s) with correct value(s) consistent with the rules.\"\n",
        "ex_fill_input = '{\"race\": \"Lightfoot Halfling\", \"class\": \"Rogue\", \"level\": 1, \"subclass\": null, \"alignment\": null, \"stats\": {\"str\": 8, \"dex\": 17, \"con\": 14, \"int\": 13, \"wis\": 12, \"cha\": 11}}'\n",
        "ex_fill_output = \"\"\"```json\n",
        "{\n",
        "  \"race\": \"Lightfoot Halfling\",\n",
        "  \"class\": \"Rogue\",\n",
        "  \"level\": 1,\n",
        "  \"subclass\": null,\n",
        "  \"alignment\": \"Chaotic Neutral\",\n",
        "  \"stats\": {\"str\": 8, \"dex\": 17, \"con\": 14, \"int\": 13, \"wis\": 12, \"cha\": 11},\n",
        "  \"hp\": 10,\n",
        "  \"ac\": 14,\n",
        "  \"skills\": [\"Stealth\", \"Acrobatics\", \"Sleight of Hand\", \"Deception\"],\n",
        "  \"weapons\": [\"Dagger\", \"Shortbow\"]\n",
        "}\n",
        "```\"\"\"\n",
        "\n",
        "# REFUSAL\n",
        "ex_ref_instr = \"Generate a complete D&D 5e character sheet based on the provided attributes.\"\n",
        "ex_ref_input = '{\"race\": \"Saiyan\", \"class\": \"Super Warrior\", \"level\": 9000, \"stats\": {\"str\": 100}}'\n",
        "ex_ref_output = \"\"\"```json\n",
        "{\n",
        "  \"message\": \"The provided character data contains invalid, homebrew, or rule-breaking content inconsistent with D&D 5e rules. Cannot generate a character sheet.\"\n",
        "}\n",
        "```\"\"\"\n",
        "\n",
        "# map task -> example\n",
        "TASK_EXAMPLES = {\n",
        "    \"generation\": (ex_gen_instr, ex_gen_input, ex_gen_output),\n",
        "    \"completion\": (ex_fill_instr, ex_fill_input, ex_fill_output),\n",
        "    \"refusal\":    (ex_ref_instr, ex_ref_input, ex_ref_output)\n",
        "}\n",
        "\n",
        "# INFERENCE\n",
        "print(f\"\\nStart inference on: {TEST_DATA_PATH}\")\n",
        "\n",
        "with open(TEST_DATA_PATH, 'r') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "results = []\n",
        "gen_kwargs = {\n",
        "    \"max_new_tokens\": 1024,\n",
        "    \"use_cache\": True,\n",
        "    \"temperature\": 0.1,\n",
        "    \"min_p\": 0.05,\n",
        "    \"repetition_penalty\": 1.1\n",
        "}\n",
        "\n",
        "for item in tqdm(test_data, desc=\"Elaborazione\"):\n",
        "    task_type = item.get(\"task_type\", \"generation\")\n",
        "    raw_prompt = item['llm_prompt']\n",
        "\n",
        "    # Parsing Input\n",
        "    if \"Input Data:\" in raw_prompt:\n",
        "        parts = raw_prompt.split(\"Input Data:\")\n",
        "        instr, inp_data = parts[0].strip(), parts[1].strip()\n",
        "    elif \"Character Sheet:\" in raw_prompt:\n",
        "        parts = raw_prompt.split(\"Character Sheet:\")\n",
        "        instr, inp_data = parts[0].strip(), parts[1].strip()\n",
        "    else:\n",
        "        instr, inp_data = \"Generate a D&D 5e character sheet.\", raw_prompt\n",
        "\n",
        "    example_tuple = TASK_EXAMPLES.get(task_type, TASK_EXAMPLES[\"generation\"])\n",
        "    one_shot_block = alpaca_template.format(*example_tuple)\n",
        "\n",
        "    # final prompt\n",
        "    current_query = alpaca_template.format(instr, inp_data, \"\")\n",
        "    full_prompt = one_shot_block + \"\\n\\n\" + current_query\n",
        "\n",
        "    # Tokenization\n",
        "    inputs = tokenizer([full_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generation\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, **gen_kwargs)\n",
        "\n",
        "    input_len = inputs.input_ids.shape[1]\n",
        "    decoded = tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)\n",
        "\n",
        "    results.append({\n",
        "        \"task_type\": task_type,\n",
        "        \"input_prompt\": raw_prompt,\n",
        "        \"generated_response\": decoded.strip(),\n",
        "        \"expected_output\": item.get(\"expected_output\"),\n",
        "        \"mode\": \"one-shot-smart\"\n",
        "    })\n",
        "\n",
        "    # Save during inference\n",
        "    if len(results) % 20 == 0:\n",
        "        with open(OUTPUT_FILE, 'w') as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "\n",
        "# final save\n",
        "with open(OUTPUT_FILE, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"\\nDone, check {OUTPUT_FILE}\")"
      ],
      "metadata": {
        "id": "v50pssh81IIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One Shot Inference on base model"
      ],
      "metadata": {
        "id": "Qq2m6ozn_gjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import json\n",
        "import gc\n",
        "from unsloth import FastLanguageModel\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# PATH\n",
        "BASE_DIR = \"/content/drive/MyDrive/DnD_Project_Data\"\n",
        "MODEL_NAME = \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "TEST_DATA_PATH = os.path.join(BASE_DIR, \"processed_dataset\", \"dnd_test.json\")\n",
        "OUTPUT_FILE = os.path.join(BASE_DIR, \"inference_results_base_one_shot\", \"predictions_Llama_1B_BASE_OneShot.json\")\n",
        "os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
        "#clean memory\n",
        "try:\n",
        "    del model\n",
        "    del tokenizer\n",
        "except:\n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Load model\n",
        "print(f\"Loading Base Model: {MODEL_NAME}...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = MODEL_NAME,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "alpaca_template = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{}\\n\\n### Input:\\n{}\\n\\n### Response:\\n{}\"\n",
        "\n",
        "# Generation Example\n",
        "ex_gen_instr = \"Generate a complete D&D 5e character sheet based on the provided attributes.\"\n",
        "ex_gen_input = '{\"race\": \"Hill Dwarf\", \"class\": \"Cleric\", \"level\": 1, \"background\": \"Acolyte\", \"stats\": {\"str\": 14, \"dex\": 8, \"con\": 15, \"int\": 10, \"wis\": 16, \"cha\": 12}}'\n",
        "ex_gen_output = \"\"\"```json\n",
        "{\n",
        "  \"race\": \"Hill Dwarf\",\n",
        "  \"subrace\": null,\n",
        "  \"class\": \"Cleric\",\n",
        "  \"subclass\": \"Life Domain\",\n",
        "  \"level\": 1,\n",
        "  \"background\": \"Acolyte\",\n",
        "  \"stats\": {\"str\": 14, \"dex\": 8, \"con\": 15, \"int\": 10, \"wis\": 16, \"cha\": 12},\n",
        "  \"hp\": 11,\n",
        "  \"ac\": 16,\n",
        "  \"alignment\": \"Lawful Good\",\n",
        "  \"skills\": [\"Insight\", \"Religion\", \"Medicine\"],\n",
        "  \"weapons\": [\"Warhammer\", \"Light Crossbow\"],\n",
        "  \"spells\": [\"Bless\", \"Cure Wounds\", \"Sacred Flame\"],\n",
        "  \"feats\": []\n",
        "}\n",
        "```\"\"\"\n",
        "\n",
        "# Completion Example\n",
        "ex_fill_instr = \"Complete this D&D 5e character sheet. Replace the NULL value(s) with correct value(s) consistent with the rules.\"\n",
        "ex_fill_input = '{\"race\": \"Lightfoot Halfling\", \"class\": \"Rogue\", \"level\": 1, \"subclass\": null, \"alignment\": null, \"stats\": {\"str\": 8, \"dex\": 17, \"con\": 14, \"int\": 13, \"wis\": 12, \"cha\": 11}}'\n",
        "ex_fill_output = \"\"\"```json\n",
        "{\n",
        "  \"race\": \"Lightfoot Halfling\",\n",
        "  \"class\": \"Rogue\",\n",
        "  \"level\": 1,\n",
        "  \"subclass\": null,\n",
        "  \"alignment\": \"Chaotic Neutral\",\n",
        "  \"stats\": {\"str\": 8, \"dex\": 17, \"con\": 14, \"int\": 13, \"wis\": 12, \"cha\": 11},\n",
        "  \"hp\": 10,\n",
        "  \"ac\": 14,\n",
        "  \"skills\": [\"Stealth\", \"Acrobatics\", \"Sleight of Hand\", \"Deception\"],\n",
        "  \"weapons\": [\"Dagger\", \"Shortbow\"]\n",
        "}\n",
        "```\"\"\"\n",
        "\n",
        "# Refusal Example\n",
        "ex_ref_instr = \"Generate a complete D&D 5e character sheet based on the provided attributes.\"\n",
        "ex_ref_input = '{\"race\": \"Saiyan\", \"class\": \"Super Warrior\", \"level\": 9000, \"stats\": {\"str\": 100}}'\n",
        "ex_ref_output = \"\"\"```json\n",
        "{\n",
        "  \"message\": \"The provided character data contains invalid, homebrew, or rule-breaking content inconsistent with D&D 5e rules. Cannot generate a character sheet.\"\n",
        "}\n",
        "```\"\"\"\n",
        "\n",
        "TASK_EXAMPLES = {\n",
        "    \"generation\": (ex_gen_instr, ex_gen_input, ex_gen_output),\n",
        "    \"completion\": (ex_fill_instr, ex_fill_input, ex_fill_output),\n",
        "    \"refusal\":    (ex_ref_instr, ex_ref_input, ex_ref_output)\n",
        "}\n",
        "\n",
        "# INFERENCE\n",
        "print(f\"\\nStart inference on: {TEST_DATA_PATH}\")\n",
        "\n",
        "with open(TEST_DATA_PATH, 'r') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "results = []\n",
        "\n",
        "gen_kwargs = {\n",
        "    \"max_new_tokens\": 1024,\n",
        "    \"use_cache\": True,\n",
        "    \"temperature\": 0.1,\n",
        "    \"min_p\": 0.05,\n",
        "    \"repetition_penalty\": 1.1,\n",
        "    \"eos_token_id\": tokenizer.eos_token_id,\n",
        "    \"pad_token_id\": tokenizer.pad_token_id,\n",
        "}\n",
        "\n",
        "for item in tqdm(test_data, desc=\"Processing Base Model\"):\n",
        "    task_type = item.get(\"task_type\", \"generation\")\n",
        "    raw_prompt = item['llm_prompt']\n",
        "\n",
        "    # Parsing Input\n",
        "    if \"Input Data:\" in raw_prompt:\n",
        "        parts = raw_prompt.split(\"Input Data:\")\n",
        "        instr, inp_data = parts[0].strip(), parts[1].strip()\n",
        "    elif \"Character Sheet:\" in raw_prompt:\n",
        "        parts = raw_prompt.split(\"Character Sheet:\")\n",
        "        instr, inp_data = parts[0].strip(), parts[1].strip()\n",
        "    else:\n",
        "        instr, inp_data = \"Generate a D&D 5e character sheet.\", raw_prompt\n",
        "\n",
        "    # Prompt One-Shot\n",
        "    example_tuple = TASK_EXAMPLES.get(task_type, TASK_EXAMPLES[\"generation\"])\n",
        "    one_shot_block = alpaca_template.format(*example_tuple)\n",
        "\n",
        "    current_query = alpaca_template.format(instr, inp_data, \"\")\n",
        "    full_prompt = one_shot_block + \"\\n\\n\" + current_query\n",
        "\n",
        "    # Tokenization\n",
        "    inputs = tokenizer([full_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generation\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, **gen_kwargs)\n",
        "\n",
        "    input_len = inputs.input_ids.shape[1]\n",
        "    decoded = tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)\n",
        "\n",
        "    results.append({\n",
        "        \"task_type\": task_type,\n",
        "        \"input_prompt\": raw_prompt,\n",
        "        \"generated_response\": decoded.strip(),\n",
        "        \"expected_output\": item.get(\"expected_output\"),\n",
        "        \"mode\": \"base-model-oneshot\"\n",
        "    })\n",
        "\n",
        "    # Save checkpoint\n",
        "    if len(results) % 20 == 0:\n",
        "        with open(OUTPUT_FILE, 'w') as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "\n",
        "# Final save\n",
        "with open(OUTPUT_FILE, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"\\nDone. Results saved to {OUTPUT_FILE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxoU1bRy_gQn",
        "outputId": "c1159b26-ff9f-4a2e-f9d4-d315c657c994"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loading Base Model: unsloth/Llama-3.2-1B-Instruct...\n",
            "==((====))==  Unsloth 2026.2.1: Fast Llama patching. Transformers: 4.57.6.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.563 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "\n",
            "Start inference on: /content/drive/MyDrive/DnD_Project_Data/processed_dataset/dnd_test.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Base Model: 100%|██████████| 636/636 [54:34<00:00,  5.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Done. Results saved to /content/drive/MyDrive/DnD_Project_Data/inference_results/predictions_Llama_1B_BASE_OneShot.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import collections\n",
        "from glob import glob\n",
        "from google.colab import drive\n",
        "\n",
        "# PATHS\n",
        "BASE_DIR = \"/content/drive/MyDrive/DnD_Project_Data\"\n",
        "INPUT_POS_INT = os.path.join(BASE_DIR, \"dataset_integrated\", \"dataset_integrated_positive.json\")\n",
        "\n",
        "SUBCLASS_UNLOCK_LEVELS = {\n",
        "    \"cleric\": 1, \"sorcerer\": 1, \"warlock\": 1,\n",
        "    \"druid\": 2, \"wizard\": 2,\n",
        "    \"default\": 3\n",
        "}\n",
        "\n",
        "# KNOWLEDGE BASE\n",
        "print(\"Building Knowledge Base...\")\n",
        "whitelists = {\n",
        "    \"races\": set(), \"classes\": set(), \"subclasses\": set(),\n",
        "    \"backgrounds\": set(), \"feats\": set(), \"skills\": set(), \"spells\": set()\n",
        "}\n",
        "\n",
        "def super_normalize(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    return re.sub(r'[^a-z0-9]', '', text.lower())\n",
        "\n",
        "try:\n",
        "    with open(INPUT_POS_INT, 'r') as f:\n",
        "        clean_data = json.load(f)\n",
        "    for char in clean_data:\n",
        "        if char.get('race'): whitelists['races'].add(super_normalize(char['race']))\n",
        "        if char.get('class'): whitelists['classes'].add(super_normalize(char['class']))\n",
        "        if char.get('subclass'): whitelists['subclasses'].add(super_normalize(char['subclass']))\n",
        "        if char.get('background'): whitelists['backgrounds'].add(super_normalize(char['background']))\n",
        "        for f in char.get('feats', []): whitelists['feats'].add(super_normalize(f))\n",
        "        for s in char.get('skills', []): whitelists['skills'].add(super_normalize(s))\n",
        "        for sp in char.get('spells', []): whitelists['spells'].add(super_normalize(sp))\n",
        "    print(f\"Knowledge Base Ready. Loaded {len(clean_data)} reference characters.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Warning: Reference dataset not found. Validation will be strictly rule-based without whitelist checks.\")\n",
        "\n",
        "# FUNCTIONS\n",
        "\n",
        "def extract_input_from_prompt(prompt):\n",
        "    try:\n",
        "        match = re.search(r'(?:Input Data:|Character Sheet:|### Input:)\\s*(\\{.*?\\})\\s*(?:###|$)', prompt, re.DOTALL)\n",
        "        if match: return json.loads(match.group(1))\n",
        "    except: pass\n",
        "    return {}\n",
        "\n",
        "def has_meaningful_content(val):\n",
        "    if val is None: return False\n",
        "    if isinstance(val, str):\n",
        "        norm = val.strip().lower()\n",
        "        if norm == \"\" or norm == \"null\" or norm == \"none\" or norm == \".\": return False\n",
        "    if isinstance(val, (list, dict)) and len(val) == 0: return False\n",
        "    if isinstance(val, (int, float)) and val <= 0: return False\n",
        "    return True\n",
        "\n",
        "# --- PARSING LOGIC ---\n",
        "def detect_repetition_loop(text, threshold=10):\n",
        "    if len(text) < 100: return False\n",
        "    tokens = re.split(r'\\s+|[,;\"]', text)\n",
        "    tokens = [t for t in tokens if len(t) > 2]\n",
        "    if not tokens: return False\n",
        "    counts = collections.Counter(tokens)\n",
        "    most_common, count = counts.most_common(1)[0]\n",
        "    if count > threshold:\n",
        "        if count > 15 and (count / len(tokens) > 0.1): return True\n",
        "    chunk_size = 30\n",
        "    if len(text) > chunk_size * 5:\n",
        "        sub = text[-chunk_size:]\n",
        "        if text.count(sub) > 5: return True\n",
        "    return False\n",
        "\n",
        "def extract_json_from_text(text):\n",
        "    try:\n",
        "        code_block = re.search(r'```(?:json)?\\s*(\\{.*?\\})\\s*```', text, re.DOTALL)\n",
        "        if code_block: return json.loads(code_block.group(1)), True\n",
        "        match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
        "        if match: return json.loads(match.group(0)), True\n",
        "        return json.loads(text), True\n",
        "    except:\n",
        "        return None, False\n",
        "\n",
        "def parse_text_content(text):\n",
        "    splitters = [\"### Response:\", \"Output:\", \"Response:\"]\n",
        "    search_text = text\n",
        "    for s in splitters:\n",
        "        if s in text:\n",
        "            search_text = text.split(s)[-1]\n",
        "            break\n",
        "    extracted = {\"stats\": {}}\n",
        "    patterns = {\n",
        "        \"race\":       r'Race[:\\*\\s\\-]*([^\\n\\*\\[]+)',\n",
        "        \"subrace\":    r'Subrace[:\\*\\s\\-]*([^\\n\\*\\[]+)',\n",
        "        \"class\":      r'Class[:\\*\\s\\-]*([^\\n\\*\\[]+)',\n",
        "        \"subclass\":   r'Subclass[:\\*\\s\\-]*([^\\n\\*\\[]+)',\n",
        "        \"background\": r'Background[:\\*\\s\\-]*([^\\n\\*\\[]+)',\n",
        "        \"level\":      r'Level[:\\*\\s\\-]*(\\d+)',\n",
        "        \"hp\":         r'(?:HP|Hit Points)[:\\*\\s\\-]*(\\d+)',\n",
        "        \"ac\":         r'(?:AC|Armor Class)[:\\*\\s\\-]*(\\d+)',\n",
        "        \"alignment\":  r'Alignment[:\\*\\s\\-]*([^\\n\\*\\[]+)'\n",
        "    }\n",
        "    found_keys = 0\n",
        "    for key, pat in patterns.items():\n",
        "        match = re.search(pat, search_text, re.IGNORECASE)\n",
        "        if match:\n",
        "            val = match.group(1).strip()\n",
        "            val = re.sub(r'[\\*\\[\\]]', '', val).strip()\n",
        "            if has_meaningful_content(val):\n",
        "                extracted[key] = val\n",
        "                found_keys += 1\n",
        "            if key == \"subrace\" and str(val).lower() == \"none\":\n",
        "                extracted[key] = \"None\"\n",
        "\n",
        "    stat_map = {\n",
        "        \"str\": [\"strength\", \"str\"], \"dex\": [\"dexterity\", \"dex\"], \"con\": [\"constitution\", \"con\"],\n",
        "        \"int\": [\"intelligence\", \"int\"], \"wis\": [\"wisdom\", \"wis\"], \"cha\": [\"charisma\", \"cha\"]\n",
        "    }\n",
        "    found_stats = 0\n",
        "    for short_k, aliases in stat_map.items():\n",
        "        for alias in aliases:\n",
        "            pat = r'(?:^|[\\s\\*])' + alias + r'[:\\*\\s\\-\\(]*(\\d+)'\n",
        "            match = re.search(pat, search_text, re.IGNORECASE)\n",
        "            if match:\n",
        "                extracted[\"stats\"][short_k] = int(match.group(1))\n",
        "                found_stats += 1\n",
        "                break\n",
        "\n",
        "    spells_pat = r'(?:^|\\n)(?:\\*\\*|#|\\s)*Spells[:\\*\\s\\-]*([\\s\\S]+?)(?:\\n(?:\\*\\*|#)|$)'\n",
        "    spells_match = re.search(spells_pat, search_text, re.IGNORECASE)\n",
        "    if spells_match:\n",
        "        content = spells_match.group(1).strip()\n",
        "        if len(content) > 5: extracted[\"spells\"] = [content]\n",
        "\n",
        "    if ((\"race\" in extracted or \"class\" in extracted) and found_stats >= 3):\n",
        "        return extracted, True\n",
        "    return None, False\n",
        "\n",
        "# VALIDATION LOGIC\n",
        "\n",
        "def check_mutations(input_json, output_data):\n",
        "    mutations = []\n",
        "\n",
        "    for k, v in input_json.items():\n",
        "        if k == 'stats': continue\n",
        "\n",
        "        if has_meaningful_content(v):\n",
        "            val_out = output_data.get(k)\n",
        "            if val_out is None:\n",
        "                mutations.append(f\"Field '{k}' dropped (Input: {v})\")\n",
        "            else:\n",
        "                str_v = str(v).lower().strip()\n",
        "                str_out = str(val_out).lower().strip()\n",
        "                if super_normalize(str_out) != super_normalize(str_v):\n",
        "                    mutations.append(f\"Field '{k}' mutated (In: '{v}' -> Out: '{val_out}')\")\n",
        "\n",
        "    if 'stats' in input_json and isinstance(input_json['stats'], dict):\n",
        "        out_stats = output_data.get('stats', {})\n",
        "        if not isinstance(out_stats, dict):\n",
        "            mutations.append(\"Stats block corrupted\")\n",
        "        else:\n",
        "            for sk, sv in input_json['stats'].items():\n",
        "                if has_meaningful_content(sv):\n",
        "                    out_sv = out_stats.get(sk)\n",
        "                    if out_sv is None:\n",
        "                         mutations.append(f\"Stat '{sk}' dropped\")\n",
        "                    elif str(out_sv) != str(sv):\n",
        "                        mutations.append(f\"Stat '{sk}' mutated ({sv} -> {out_sv})\")\n",
        "\n",
        "    return mutations\n",
        "\n",
        "def check_progress(input_json, output_data):\n",
        "    added = []\n",
        "    schema_keys = ['race', 'class', 'subclass', 'background', 'level', 'hp', 'ac', 'alignment', 'spells', 'subrace', 'skills', 'weapons', 'feats']\n",
        "\n",
        "    for k in schema_keys:\n",
        "        input_val = input_json.get(k)\n",
        "        if not has_meaningful_content(input_val):\n",
        "            output_val = output_data.get(k)\n",
        "            if has_meaningful_content(output_val):\n",
        "                if k == 'subclass' and str(output_val) == \".\": continue\n",
        "                added.append(k)\n",
        "\n",
        "    input_stats_empty = False\n",
        "    if 'stats' in input_json:\n",
        "        inp_s = input_json.get('stats')\n",
        "        if not has_meaningful_content(inp_s):\n",
        "            input_stats_empty = True\n",
        "            out_s = output_data.get('stats', {})\n",
        "            if out_s and len(out_s) >= 3:\n",
        "                added.append('stats')\n",
        "    else:\n",
        "        input_stats_empty = True\n",
        "        if output_data.get('stats'): added.append('stats')\n",
        "\n",
        "    return (len(added) > 0), added, input_stats_empty\n",
        "\n",
        "\n",
        "def validate_game_rules(char_json, input_json, task_type, input_stats_empty):\n",
        "    errors = []\n",
        "    passed = []\n",
        "\n",
        "    # COMPLETION task\n",
        "    is_completion = (task_type == 'completion')\n",
        "\n",
        "    race_norm = super_normalize(char_json.get('race'))\n",
        "    class_name = char_json.get('class', 'Unknown')\n",
        "    class_norm = super_normalize(class_name)\n",
        "    try: lvl = int(char_json.get('level', 1))\n",
        "    except: lvl = 1\n",
        "\n",
        "\n",
        "    # SPECIAL CHECKS: SUBRACE & SUBCLASS\n",
        "\n",
        "    # --- SUBRACE ---\n",
        "    RACES_NO_SUBRACES = {'human', 'dragonborn', 'tiefling', 'halforc', 'halfelf', 'tabaxi', 'triton'}\n",
        "    subrace_val = char_json.get('subrace')\n",
        "    subrace_input = input_json.get('subrace')\n",
        "\n",
        "    if race_norm not in RACES_NO_SUBRACES and race_norm:\n",
        "        if not has_meaningful_content(subrace_val):\n",
        "             if not has_meaningful_content(subrace_input):\n",
        "                 errors.append(f\"Missing Subrace for {char_json.get('race')}\")\n",
        "        else:\n",
        "            if not has_meaningful_content(subrace_input):\n",
        "                passed.append(f\"Generated 'subrace': '{subrace_val}'\")\n",
        "    else:\n",
        "        if has_meaningful_content(subrace_val) and str(subrace_val).lower() != \"none\":\n",
        "            errors.append(f\"Invalid Subrace '{subrace_val}' for {char_json.get('race')}\")\n",
        "\n",
        "    # --- SUBCLASS ---\n",
        "    unlock_lvl = SUBCLASS_UNLOCK_LEVELS.get(class_norm, SUBCLASS_UNLOCK_LEVELS[\"default\"])\n",
        "    subclass_val = char_json.get('subclass')\n",
        "    subclass_input = input_json.get('subclass')\n",
        "\n",
        "    if lvl >= unlock_lvl:\n",
        "        if not has_meaningful_content(subclass_val):\n",
        "            if not has_meaningful_content(subclass_input):\n",
        "                errors.append(f\"Missing Subclass (Required for {class_name} at Lvl {lvl})\")\n",
        "        else:\n",
        "            if not has_meaningful_content(subclass_input):\n",
        "                passed.append(f\"Generated 'subclass': '{subclass_val}'\")\n",
        "    else:\n",
        "        if has_meaningful_content(subclass_val) and str(subclass_val).lower() != \"none\":\n",
        "            errors.append(f\"Invalid Subclass '{subclass_val}' (Unlocks at Lvl {unlock_lvl}, Char is Lvl {lvl})\")\n",
        "\n",
        "\n",
        "    # GENERIC CHECKS: ALIGNMENT, BG, HP, AC\n",
        "    check_fields = ['hp', 'background', 'ac', 'alignment']\n",
        "\n",
        "    for field in check_fields:\n",
        "        out_val = char_json.get(field)\n",
        "        in_val = input_json.get(field)\n",
        "\n",
        "        if has_meaningful_content(out_val):\n",
        "            if not has_meaningful_content(in_val):\n",
        "                passed.append(f\"Generated '{field}'\")\n",
        "        else:\n",
        "            if not has_meaningful_content(in_val):\n",
        "                if is_completion:\n",
        "                    errors.append(f\"Failed to complete field '{field}'\")\n",
        "                elif field in ['hp', 'background']:\n",
        "                    errors.append(f\"Missing {field.capitalize()}\")\n",
        "\n",
        "    # STATS CHECK\n",
        "    stats = char_json.get('stats')\n",
        "\n",
        "    if has_meaningful_content(stats):\n",
        "        # Stats present in the output\n",
        "        stat_valid = True\n",
        "        for k, v in stats.items():\n",
        "            try:\n",
        "                if int(v) < 1 or int(v) > 30:\n",
        "                    errors.append(f\"Stat '{k}' out of range ({v})\")\n",
        "                    stat_valid = False\n",
        "            except: pass\n",
        "\n",
        "        # if were null in input -> model filled the field\n",
        "        if stat_valid and input_stats_empty:\n",
        "            passed.append(\"Generated 'stats' correctly\")\n",
        "\n",
        "    else:\n",
        "        # missing stats in output\n",
        "        if input_stats_empty:\n",
        "            # empty input -> empty output\n",
        "            if is_completion:\n",
        "                # if completion -> must fill\n",
        "                errors.append(\"Failed to complete field 'stats'\")\n",
        "            else:\n",
        "                # In generation. all characters have stats in input\n",
        "                errors.append(\"Missing Stats\")\n",
        "        else:\n",
        "            errors.append(\"Stats dropped from input\")\n",
        "\n",
        "    # SPELLS CHECK\n",
        "    full_casters = {'bard', 'cleric', 'druid', 'sorcerer', 'warlock', 'wizard'}\n",
        "    half_casters = {'paladin', 'ranger'}\n",
        "    should_have_magic = (class_norm in full_casters) or (class_norm in half_casters and lvl >= 2)\n",
        "    spells_val = char_json.get('spells', [])\n",
        "\n",
        "    if should_have_magic:\n",
        "        if has_meaningful_content(spells_val):\n",
        "             if not has_meaningful_content(input_json.get('spells')):\n",
        "                 passed.append(\"Generated 'spells'\")\n",
        "        else:\n",
        "             if not has_meaningful_content(input_json.get('spells')):\n",
        "                 errors.append(f\"Missing Spells (Rule: {class_name} at Lvl {lvl})\")\n",
        "\n",
        "    return errors, passed\n",
        "def detect_refusal(raw_text, json_obj=None):\n",
        "    clean_text = super_normalize(raw_text)\n",
        "    refusal_keywords = [\n",
        "        # Keywords\n",
        "        \"cannotgenerate\", \"unabletofulfill\", \"cannotcomplete\",\n",
        "        \"invalidhomebrew\", \"rulebreakingcontent\", \"inconsistentwithdnd\",\n",
        "        \"safetyguidelines\", \"violatespolicy\", \"cannotcreatecharactersheet\",\n",
        "        \"contentpolicy\", \"cannotcompletethisrequest\", \"icannot\", \"sorrybut\",\n",
        "        \"notcontainenoughinformation\", \"insufficientinformation\",\n",
        "        \"pleaseprovideallrelevant\", \"missingrequired\", \"notenoughinformation\",\n",
        "        \"invalidcharactersheet\", \"invalidformat\",\n",
        "        \"providevalidattribute\", \"providevalidvalue\",\n",
        "        \"missingstats\", \"statscannotbenull\"\n",
        "    ]\n",
        "\n",
        "    if any(k in clean_text for k in refusal_keywords): return True\n",
        "\n",
        "    if json_obj:\n",
        "        keys = set(json_obj.keys())\n",
        "        # Check standard message {data: null, message: ...}\n",
        "        if \"data\" in keys and json_obj[\"data\"] is None and \"message\" in keys: return True\n",
        "\n",
        "        if \"message\" in keys and isinstance(json_obj[\"message\"], str):\n",
        "            clean_msg = super_normalize(json_obj[\"message\"])\n",
        "            if any(k in clean_msg for k in refusal_keywords): return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# FILE ANALYSIS\n",
        "\n",
        "def analyze_predictions(file_path):\n",
        "    model_name = os.path.basename(file_path).replace(\"predictions_\", \"\").replace(\".json\", \"\")\n",
        "    print(f\"Analyzing {model_name}...\")\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r') as f: predictions = json.load(f)\n",
        "    except:\n",
        "        return None, None, None, None, None\n",
        "\n",
        "    stats_counter = {\"GEN\": collections.Counter(), \"COMP\": collections.Counter(), \"REF\": collections.Counter()}\n",
        "    detail_stats = {\"missed_fields\": collections.Counter(), \"valid_fields\": collections.Counter()}\n",
        "    format_stats = {\"valid_json\": 0, \"valid_text_parsed\": 0, \"garbage\": 0, \"total\": 0}\n",
        "\n",
        "    log_lines = []\n",
        "\n",
        "    for idx, entry in enumerate(predictions):\n",
        "        task_type = entry.get('task_type', 'generation')\n",
        "        prompt = entry.get('input_prompt', '')\n",
        "        response = entry.get('generated_response', '')\n",
        "\n",
        "        input_json = extract_input_from_prompt(prompt)\n",
        "\n",
        "        json_parsed, is_json = extract_json_from_text(response)\n",
        "        text_parsed, is_text = parse_text_content(response)\n",
        "\n",
        "        is_loop = detect_repetition_loop(response)\n",
        "\n",
        "        format_stats[\"total\"] += 1\n",
        "\n",
        "        if is_json:\n",
        "            format_stats[\"valid_json\"] += 1\n",
        "            data_object = json_parsed\n",
        "            parsing_source = \"JSON\"\n",
        "        elif is_text:\n",
        "            format_stats[\"valid_text_parsed\"] += 1\n",
        "            data_object = text_parsed\n",
        "            parsing_source = \"TEXT_PARSER\"\n",
        "        else:\n",
        "            format_stats[\"garbage\"] += 1\n",
        "            data_object = None\n",
        "            parsing_source = \"NONE\"\n",
        "\n",
        "        status = \"UNKNOWN\"\n",
        "        reasoning = []\n",
        "        passed_checks = []\n",
        "\n",
        "        is_refusal = detect_refusal(response, json_parsed)\n",
        "\n",
        "        if task_type == 'refusal':\n",
        "            if is_refusal:\n",
        "                status = \"SUCCESS\"\n",
        "                stats_counter[\"REF\"][\"SUCCESS\"] += 1\n",
        "            else:\n",
        "                status = \"FAILURE\"\n",
        "                stats_counter[\"REF\"][\"FAILURE\"] += 1\n",
        "                reasoning.append(\"Model failed to refuse.\")\n",
        "\n",
        "        else:\n",
        "            cat = \"GEN\" if task_type == \"generation\" else \"COMP\"\n",
        "\n",
        "            if is_refusal:\n",
        "                status = \"FAIL_REFUSAL\"\n",
        "                stats_counter[cat][\"FAIL_REFUSAL\"] += 1\n",
        "                reasoning.append(\"Model refused a valid request.\")\n",
        "            elif data_object is None:\n",
        "                if is_loop:\n",
        "                    status = \"FAIL_LOOP\"\n",
        "                    stats_counter[cat][\"FAIL_LOOP\"] += 1\n",
        "                    reasoning.append(\"Model degenerated into loop (Garbage).\")\n",
        "                else:\n",
        "                    status = \"FAIL_FORMAT_EMPTY\"\n",
        "                    stats_counter[cat][\"FAIL_FORMAT_EMPTY\"] += 1\n",
        "                    reasoning.append(\"No valid data found.\")\n",
        "            else:\n",
        "                mutations = check_mutations(input_json, data_object)\n",
        "\n",
        "                if mutations:\n",
        "                    status = \"FAIL_MUTATION\"\n",
        "                    stats_counter[cat][\"FAIL_MUTATION\"] += 1\n",
        "                    reasoning.extend(mutations)\n",
        "                else:\n",
        "                    has_progress, added_fields, input_stats_empty = check_progress(input_json, data_object)\n",
        "\n",
        "                    if not has_progress:\n",
        "                        status = \"FAIL_STAGNATION\"\n",
        "                        stats_counter[cat][\"FAIL_STAGNATION\"] += 1\n",
        "                        reasoning.append(\"Echoed input. No new fields generated.\")\n",
        "                    else:\n",
        "                        passed_checks.append(f\"Added/Generated fields: {added_fields}\")\n",
        "\n",
        "                        errs, pass_c = validate_game_rules(data_object, input_json, task_type, input_stats_empty)\n",
        "                        passed_checks.extend(pass_c)\n",
        "\n",
        "                        if errs:\n",
        "                            status = \"PARTIAL_SUCCESS\"\n",
        "                            stats_counter[cat][\"PARTIAL_SUCCESS\"] += 1\n",
        "                            reasoning.extend(errs)\n",
        "                        else:\n",
        "                            status = \"SUCCESS\"\n",
        "                            stats_counter[cat][\"SUCCESS\"] += 1\n",
        "\n",
        "                if is_loop and status in [\"SUCCESS\", \"PARTIAL_SUCCESS\"]:\n",
        "                    status = \"PARTIAL_SUCCESS\"\n",
        "                    reasoning.append(\"WARNING: Repetition loop detected in output tail.\")\n",
        "                    if status == \"SUCCESS\":\n",
        "                         stats_counter[cat][\"SUCCESS\"] -= 1\n",
        "                         stats_counter[cat][\"PARTIAL_SUCCESS\"] += 1\n",
        "\n",
        "        def pj(d): return json.dumps(d, indent=2) if isinstance(d, dict) else str(d)\n",
        "        log_entry = (\n",
        "            f\"{'='*80}\\nENTRY #{idx+1} | TASK: {task_type.upper()} | STATUS: {status} | FORMAT: {parsing_source}\\n{'-'*80}\\n\"\n",
        "            f\"INPUT:\\n{pj(input_json)}\\n\\n\"\n",
        "            f\"EXTRACTED DATA ({parsing_source}):\\n{pj(data_object) if data_object else 'NONE'}\\n\\n\"\n",
        "            f\"RAW MODEL RESPONSE:\\n{response}\\n\\n\"\n",
        "            f\"REASONING:\\n\" + \"\\n\".join([f\"  [!] {r}\" for r in reasoning]) + \"\\n\" +\n",
        "            \"\\n\".join([f\"  [OK] {c}\" for c in passed_checks]) + \"\\n\\n\"\n",
        "        )\n",
        "        log_lines.append(log_entry)\n",
        "\n",
        "    return model_name, stats_counter, detail_stats, format_stats, log_lines\n",
        "\n",
        "# REPORT\n",
        "\n",
        "def generate_report(target_dir_path):\n",
        "    print(f\"\\nSTARTING VALIDATION FOR DIRECTORY: {target_dir_path}\")\n",
        "    if not os.path.exists(target_dir_path): return\n",
        "    output_report_file = os.path.join(target_dir_path, \"VALIDATION_REPORT.txt\")\n",
        "    log_dir = os.path.join(target_dir_path, \"validation_logs\")\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    files = glob(os.path.join(target_dir_path, \"predictions_*.json\"))\n",
        "\n",
        "    report = [\"VALIDATION REPORT (V15 - Full RAW LOGS Included)\", f\"Directory: {target_dir_path}\", \"=\"*80]\n",
        "\n",
        "    for f in files:\n",
        "        name, s, d, f_stats, logs = analyze_predictions(f)\n",
        "        if not name: continue\n",
        "        with open(os.path.join(log_dir, f\"log_{name}.txt\"), 'w') as logf: logf.write(\"\\n\".join(logs))\n",
        "\n",
        "        report.append(f\"\\nMODEL: {name}\\n\" + \"-\"*40)\n",
        "        report.append(\"[FORMAT COMPLIANCE]\")\n",
        "        tot = f_stats[\"total\"]\n",
        "        if tot:\n",
        "            report.append(f\"  Valid JSON: {f_stats['valid_json']} ({f_stats['valid_json']/tot*100:.1f}%)\")\n",
        "\n",
        "        report.append(\"\\n[SUBSTANCE & RULES PERFORMANCE]\")\n",
        "        for t in [\"GEN\", \"COMP\", \"REF\"]:\n",
        "            tot_task = sum(s[t].values())\n",
        "            report.append(f\"  {t} TASK (Total: {tot_task})\")\n",
        "            for k, v in sorted(s[t].items()):\n",
        "                report.append(f\"    - {k:<20}: {v} ({v/tot_task*100:.1f}%)\" if tot_task else f\"    - {k}: 0\")\n",
        "\n",
        "    with open(output_report_file, 'w') as f: f.write(\"\\n\".join(report))\n",
        "    print(f\"Report generated: {output_report_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waHfGTRF7JcV",
        "outputId": "4c484bed-6e5f-4ff1-c187-585734e8a189"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building Knowledge Base...\n",
            "Knowledge Base Ready. Loaded 2114 reference characters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION: BASELINE MODELS\n",
        "# Path\n",
        "BASELINE_DIR = os.path.join(BASE_DIR, \"inference_results_baseline\")\n",
        "generate_report(BASELINE_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir-vYqhV7fUA",
        "outputId": "32a477db-3798-4111-9a36-67aa67219f9e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STARTING VALIDATION FOR DIRECTORY: /content/drive/MyDrive/DnD_Project_Data/inference_results_baseline\n",
            "Analyzing gemma-2-2b-it_BASELINE...\n",
            "Analyzing Qwen3-0.6B_BASELINE...\n",
            "Analyzing Llama-3.2-1B-Instruct_BASELINE...\n",
            "Report generated: /content/drive/MyDrive/DnD_Project_Data/inference_results_baseline/VALIDATION_REPORT.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION: FINETUNED (ZERO SHOT)\n",
        "# Path\n",
        "FT_ZEROSHOT_DIR = os.path.join(BASE_DIR, \"inference_results_finetuned&zero_shot\")\n",
        "\n",
        "generate_report(FT_ZEROSHOT_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CCPDlK57jQK",
        "outputId": "9c505201-e148-4f4d-fbd0-4afd71d7c68d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STARTING VALIDATION FOR DIRECTORY: /content/drive/MyDrive/DnD_Project_Data/inference_results_finetuned&zero_shot\n",
            "Analyzing gemma-2-2b-it...\n",
            "Analyzing Llama-3.2-1B-Instruct...\n",
            "Analyzing Qwen3-0.6B...\n",
            "Report generated: /content/drive/MyDrive/DnD_Project_Data/inference_results_finetuned&zero_shot/VALIDATION_REPORT.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION: FINETUNED (ONE SHOT)\n",
        "# Path\n",
        "\n",
        "ONE_SHOT_DIR = os.path.join(BASE_DIR, \"inference_results_finetuned&one_shot\")\n",
        "\n",
        "generate_report(ONE_SHOT_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D21ezqjd7nxc",
        "outputId": "01258c7a-9ebd-4788-a82b-515c0aae5851"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STARTING VALIDATION FOR DIRECTORY: /content/drive/MyDrive/DnD_Project_Data/inference_results_finetuned&one_shot\n",
            "Analyzing Llama_1B_OneShot_Smart...\n",
            "Report generated: /content/drive/MyDrive/DnD_Project_Data/inference_results_finetuned&one_shot/VALIDATION_REPORT.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION: BASE (ONE SHOT)\n",
        "# Path\n",
        "\n",
        "BASE_ONE_SHOT_DIR = os.path.join(BASE_DIR, \"inference_results_base_one_shot\")\n",
        "\n",
        "generate_report(BASE_ONE_SHOT_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbvbFQr_aIiI",
        "outputId": "866a128c-2f51-4751-b213-426d2009f9ce"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STARTING VALIDATION FOR DIRECTORY: /content/drive/MyDrive/DnD_Project_Data/inference_results_base_one_shot\n",
            "Analyzing Llama_1B_BASE_OneShot...\n",
            "Report generated: /content/drive/MyDrive/DnD_Project_Data/inference_results_base_one_shot/VALIDATION_REPORT.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "# PATHS\n",
        "BASE_DIR = \"/content/drive/MyDrive/DnD_Project_Data\"\n",
        "\n",
        "REPORT_PATHS = {\n",
        "    \"BASELINE\": os.path.join(BASE_DIR, \"inference_results_baseline\", \"VALIDATION_REPORT.txt\"),\n",
        "    \"BASE_ONE_SHOT\": os.path.join(BASE_DIR, \"inference_results_base_one_shot\", \"VALIDATION_REPORT.txt\"),\n",
        "    \"FINETUNED_ZERO_SHOT\": os.path.join(BASE_DIR, \"inference_results_finetuned&zero_shot\", \"VALIDATION_REPORT.txt\"),\n",
        "    \"FINETUNED_ONE_SHOT\": os.path.join(BASE_DIR, \"inference_results_finetuned&one_shot\", \"VALIDATION_REPORT.txt\")\n",
        "}\n",
        "\n",
        "OUTPUT_MASTER_REPORT = os.path.join(BASE_DIR, \"PROJECT_FINAL_COMPARISON_REPORT.txt\")\n",
        "\n",
        "# DEFINITIONS\n",
        "METRICS_DEF = \"\"\"\n",
        "[METRIC DEFINITIONS]\n",
        "- Valid JSON: Percentage of outputs that are syntactically correct JSON.\n",
        "- GEN Full (Generation Success): Percentage of generated characters that strictly follow all D&D 5e rules (Stats, Spells, Class reqs).\n",
        "- GEN Part (Generation Partial): Output is valid JSON and has new content, but missed a specific rule (e.g., missing Subclass).\n",
        "- COMP Full (Completion Success): Percentage of correctly filled NULL fields without dropping existing data.\n",
        "- COMP Part (Completion Partial): Fields were filled, but with minor errors or inconsistencies (e.g., wrong skill for background).\n",
        "- REF Full (Refusal Success): Percentage of successful refusals for unsafe/invalid prompts.\n",
        "\"\"\"\n",
        "\n",
        "ERRORS_DEF = \"\"\"\n",
        "[ERROR TYPE DEFINITIONS]\n",
        "- FAIL_FORMAT_EMPTY: The model produced no extractable data or empty JSON.\n",
        "- FAIL_LOOP: The model entered a repetition loop (repetition penalty failed).\n",
        "- FAIL_MUTATION: The model hallucinated changes to immutable input data (e.g., changed 'Elf' to 'Human' in completion).\n",
        "- FAIL_STAGNATION: The model echoed the input without generating new fields (Completion task).\n",
        "- FAIL_REFUSAL: The model failed to refuse an invalid request.\n",
        "- FAIL_COMPLETION: The model left target fields NULL in a completion task.\n",
        "\"\"\"\n",
        "\n",
        "# PARSING FUNCTION\n",
        "def parse_validation_report(file_path, mode_label):\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Warning: Report not found at {file_path}\")\n",
        "        return []\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # Split by model sections\n",
        "    sections = content.split(\"MODEL: \")[1:]\n",
        "    data = []\n",
        "\n",
        "    for sec in sections:\n",
        "        lines = sec.split('\\n')\n",
        "        model_name = lines[0].strip()\n",
        "\n",
        "        # Structure for the summary table\n",
        "        entry = {\n",
        "            \"Model\": model_name,\n",
        "            \"Mode\": mode_label,\n",
        "            \"Valid JSON\": \"0.0%\",\n",
        "            \"GEN Full\": \"0.0%\",  \"GEN Part\": \"0.0%\",\n",
        "            \"COMP Full\": \"0.0%\", \"COMP Part\": \"0.0%\",\n",
        "            \"REF Full\": \"0.0%\"\n",
        "        }\n",
        "\n",
        "        # Structure for detailed analysis\n",
        "        details = {\n",
        "            \"Model\": model_name,\n",
        "            \"Mode\": mode_label,\n",
        "            \"GEN\": {},\n",
        "            \"COMP\": {},\n",
        "            \"REF\": {}\n",
        "        }\n",
        "\n",
        "        # Extract JSON Stat\n",
        "        json_match = re.search(r\"Valid JSON:\\s+\\d+\\s+\\(([\\d\\.]+%)\\)\", sec)\n",
        "        if json_match: entry[\"Valid JSON\"] = json_match.group(1)\n",
        "\n",
        "        # Helper to extract task blocks\n",
        "        def get_task_block(text, task_name):\n",
        "            start = text.find(f\"{task_name} TASK\")\n",
        "            if start == -1: return \"\"\n",
        "            return text[start:]\n",
        "\n",
        "        gen_block = get_task_block(sec, \"GEN\")\n",
        "        comp_block = get_task_block(sec, \"COMP\")\n",
        "        ref_block = get_task_block(sec, \"REF\")\n",
        "\n",
        "        # Cut blocks to avoid overlap\n",
        "        if \"COMP TASK\" in gen_block: gen_block = gen_block.split(\"COMP TASK\")[0]\n",
        "        if \"REF TASK\" in comp_block: comp_block = comp_block.split(\"REF TASK\")[0]\n",
        "\n",
        "        # Extract Values\n",
        "        def extract_stats(block, target_dict_key, data_entry, prefix):\n",
        "            # Extract Table Percentages\n",
        "            s_match = re.search(r\"-\\s+SUCCESS\\s+:\\s+\\d+\\s+\\(([\\d\\.]+%)\\)\", block)\n",
        "            p_match = re.search(r\"-\\s+PARTIAL_SUCCESS\\s+:\\s+\\d+\\s+\\(([\\d\\.]+%)\\)\", block)\n",
        "\n",
        "            if prefix == \"REF\":\n",
        "                data_entry[f\"{prefix} Full\"] = s_match.group(1) if s_match else \"0.0%\"\n",
        "            else:\n",
        "                data_entry[f\"{prefix} Full\"] = s_match.group(1) if s_match else \"0.0%\"\n",
        "                data_entry[f\"{prefix} Part\"] = p_match.group(1) if p_match else \"0.0%\"\n",
        "\n",
        "            # Extract All Specific Counters for Detailed Report\n",
        "            all_matches = re.findall(r\"-\\s+([A-Z_]+)\\s+:\\s+(\\d+)\\s+\", block)\n",
        "            for k, v in all_matches:\n",
        "                details[target_dict_key][k] = int(v)\n",
        "\n",
        "        extract_stats(gen_block, \"GEN\", entry, \"GEN\")\n",
        "        extract_stats(comp_block, \"COMP\", entry, \"COMP\")\n",
        "        extract_stats(ref_block, \"REF\", entry, \"REF\")\n",
        "\n",
        "        data.append((entry, details))\n",
        "\n",
        "    return data\n",
        "\n",
        "# AGGREGATION\n",
        "all_parsed = []\n",
        "\n",
        "# Load all reports with correct labels\n",
        "all_parsed.extend(parse_validation_report(REPORT_PATHS[\"BASELINE\"], \"Baseline (Zero-Shot)\"))\n",
        "all_parsed.extend(parse_validation_report(REPORT_PATHS[\"BASE_ONE_SHOT\"], \"Baseline (One-Shot)\"))\n",
        "all_parsed.extend(parse_validation_report(REPORT_PATHS[\"FINETUNED_ZERO_SHOT\"], \"Fine-Tuned (Zero-Shot)\"))\n",
        "all_parsed.extend(parse_validation_report(REPORT_PATHS[\"FINETUNED_ONE_SHOT\"], \"Fine-Tuned (One-Shot)\"))\n",
        "\n",
        "# Separate table data from details\n",
        "table_data = [x[0] for x in all_parsed]\n",
        "details_data = [x[1] for x in all_parsed]\n",
        "\n",
        "# Sorting Logic\n",
        "mode_order = {\n",
        "    \"Baseline (Zero-Shot)\": 0,\n",
        "    \"Baseline (One-Shot)\": 1,\n",
        "    \"Fine-Tuned (Zero-Shot)\": 2,\n",
        "    \"Fine-Tuned (One-Shot)\": 3\n",
        "}\n",
        "\n",
        "# Sort by Model Name then by Mode\n",
        "table_data.sort(key=lambda x: (x['Model'].split('_')[0], mode_order.get(x['Mode'], 99)))\n",
        "details_data.sort(key=lambda x: (x['Model'].split('_')[0], mode_order.get(x['Mode'], 99)))\n",
        "\n",
        "# REPORT GENERATION\n",
        "report_lines = []\n",
        "report_lines.append(\"=\"*100)\n",
        "report_lines.append(\"                                PROJECT FINAL COMPARISON REPORT\")\n",
        "report_lines.append(\"=\"*100)\n",
        "report_lines.append(\"\")\n",
        "\n",
        "# DEFINITIONS\n",
        "report_lines.append(METRICS_DEF.strip())\n",
        "report_lines.append(\"\")\n",
        "report_lines.append(ERRORS_DEF.strip())\n",
        "report_lines.append(\"\")\n",
        "report_lines.append(\"=\"*100)\n",
        "report_lines.append(\"\")\n",
        "\n",
        "# DETAILED PERFORMANCE\n",
        "report_lines.append(\"[DETAILED PERFORMANCE BREAKDOWN]\")\n",
        "report_lines.append(\"Specific counts of success and failure types for each model configuration.\")\n",
        "report_lines.append(\"\")\n",
        "\n",
        "for item in details_data:\n",
        "    model_header = f\"MODEL: {item['Model']} | MODE: {item['Mode']}\"\n",
        "    report_lines.append(\"-\" * 80)\n",
        "    report_lines.append(model_header)\n",
        "    report_lines.append(\"-\" * 80)\n",
        "\n",
        "    for task in [\"GEN\", \"COMP\", \"REF\"]:\n",
        "        stats = item[task]\n",
        "        if not stats:\n",
        "            report_lines.append(f\"  {task}: No Data\")\n",
        "            continue\n",
        "\n",
        "        report_lines.append(f\"  {task} TASK:\")\n",
        "        # Sort by count descending\n",
        "        sorted_stats = sorted(stats.items(), key=lambda x: x[1], reverse=True)\n",
        "        for k, v in sorted_stats:\n",
        "            report_lines.append(f\"    - {k:<20}: {v}\")\n",
        "    report_lines.append(\"\")\n",
        "\n",
        "report_lines.append(\"=\"*100)\n",
        "report_lines.append(\"\")\n",
        "\n",
        "# COMPARISON TABLE\n",
        "report_lines.append(\"[FINAL COMPARISON TABLE]\")\n",
        "header_fmt = \"| {:<42} | {:<8} | {:<10} | {:<10} | {:<10} | {:<10} | {:<8} |\"\n",
        "divider = \"-\" * 118\n",
        "\n",
        "header = header_fmt.format(\"MODEL & MODE\", \"JSON %\", \"GEN Full\", \"GEN Part\", \"COMP Full\", \"COMP Part\", \"REF %\")\n",
        "\n",
        "report_lines.append(divider)\n",
        "report_lines.append(header)\n",
        "report_lines.append(divider)\n",
        "\n",
        "current_model_base = \"\"\n",
        "\n",
        "for row in table_data:\n",
        "    # --- FIX: LOGICA MIGLIORATA PER IL SEPARATORE ---\n",
        "    model_name = row['Model']\n",
        "\n",
        "    # Distinguiamo esplicitamente \"Llama_1B\" (Custom/Legacy) da \"Llama-3.2\" (Official)\n",
        "    # Altrimenti finiscono entrambi sotto \"Llama\" e non viene stampata la riga\n",
        "    if \"Llama_1B\" in model_name:\n",
        "        model_group = \"Llama_1B_Custom\"\n",
        "    elif \"Llama-3.2\" in model_name:\n",
        "        model_group = \"Llama_3.2_Instruct\"\n",
        "    else:\n",
        "        # Fallback per Qwen, Gemma ecc. (prende la prima parola)\n",
        "        model_group = model_name.split('_')[0].split('-')[0]\n",
        "\n",
        "    if model_group != current_model_base and current_model_base != \"\":\n",
        "        report_lines.append(f\"| {'-'*42} | {'-'*8} | {'-'*10} | {'-'*10} | {'-'*10} | {'-'*10} | {'-'*8} |\")\n",
        "\n",
        "    current_model_base = model_group\n",
        "    # -----------------------------------------------\n",
        "\n",
        "    # Shorten Mode string for table fit\n",
        "    mode_short = row['Mode'].replace('Fine-Tuned', 'FT').replace('Baseline', 'Base').replace(' (Zero-Shot)', ' ZS').replace(' (One-Shot)', ' 1S')\n",
        "\n",
        "    display_name = f\"{row['Model']} [{mode_short}]\"\n",
        "    if len(display_name) > 42: display_name = display_name[:39] + \"..\"\n",
        "\n",
        "    line = header_fmt.format(\n",
        "        display_name,\n",
        "        row['Valid JSON'],\n",
        "        row['GEN Full'],\n",
        "        row['GEN Part'],\n",
        "        row['COMP Full'],\n",
        "        row['COMP Part'],\n",
        "        row['REF Full']\n",
        "    )\n",
        "    report_lines.append(line)\n",
        "\n",
        "report_lines.append(divider)\n",
        "\n",
        "# WRITE TO FILE\n",
        "with open(OUTPUT_MASTER_REPORT, 'w') as f:\n",
        "    f.write(\"\\n\".join(report_lines))\n",
        "\n",
        "print(f\"\\nDone. Report saved to:\\n{OUTPUT_MASTER_REPORT}\")\n",
        "\n",
        "# PRINT PREVIEW TO CONSOLE\n",
        "print(\"\\n--- FINAL TABLE PREVIEW ---\\n\")\n",
        "print(divider)\n",
        "print(header)\n",
        "print(divider)\n",
        "\n",
        "current_model_base_preview = \"\"\n",
        "for row in table_data:\n",
        "    # --- STESSA LOGICA DI SEPARAZIONE PER LA PREVIEW ---\n",
        "    model_name = row['Model']\n",
        "    if \"Llama_1B\" in model_name:\n",
        "        model_group = \"Llama_1B_Custom\"\n",
        "    elif \"Llama-3.2\" in model_name:\n",
        "        model_group = \"Llama_3.2_Instruct\"\n",
        "    else:\n",
        "        model_group = model_name.split('_')[0].split('-')[0]\n",
        "\n",
        "    if model_group != current_model_base_preview and current_model_base_preview != \"\":\n",
        "        print(f\"| {'-'*42} | {'-'*8} | {'-'*10} | {'-'*10} | {'-'*10} | {'-'*10} | {'-'*8} |\")\n",
        "    current_model_base_preview = model_group\n",
        "    # ---------------------------------------------------\n",
        "\n",
        "    mode_short = row['Mode'].replace('Fine-Tuned', 'FT').replace('Baseline', 'Base').replace(' (Zero-Shot)', ' ZS').replace(' (One-Shot)', ' 1S')\n",
        "    display_name = f\"{row['Model'][:15]}.. [{mode_short}]\"\n",
        "    print(header_fmt.format(display_name, row['Valid JSON'], row['GEN Full'], row['GEN Part'], row['COMP Full'], row['COMP Part'], row['REF Full']))\n",
        "print(divider)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MKNd3KgCBAa",
        "outputId": "b044ab6a-5541-425b-ad28-451d6eab388a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Done. Report saved to:\n",
            "/content/drive/MyDrive/DnD_Project_Data/PROJECT_FINAL_COMPARISON_REPORT.txt\n",
            "\n",
            "--- FINAL TABLE PREVIEW ---\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------\n",
            "| MODEL & MODE                               | JSON %   | GEN Full   | GEN Part   | COMP Full  | COMP Part  | REF %    |\n",
            "----------------------------------------------------------------------------------------------------------------------\n",
            "| Llama_1B_BASE_O.. [Base 1S]                | 98.6%    | 43.4%      | 56.1%      | 2.4%       | 37.7%      | 90.6%    |\n",
            "| Llama_1B_OneSho.. [FT 1S]                  | 79.7%    | 59.4%      | 40.6%      | 3.3%       | 41.0%      | 100.0%   |\n",
            "| ------------------------------------------ | -------- | ---------- | ---------- | ---------- | ---------- | -------- |\n",
            "| Llama-3.2-1B-In.. [Base ZS]                | 0.0%     | 0.0%       | 11.3%      | 0.0%       | 0.0%       | 0.0%     |\n",
            "| Llama-3.2-1B-In.. [FT ZS]                  | 56.8%    | 0.0%       | 1.4%       | 12.7%      | 41.5%      | 0.5%     |\n",
            "| ------------------------------------------ | -------- | ---------- | ---------- | ---------- | ---------- | -------- |\n",
            "| Qwen3-0.6B_BASE.. [Base ZS]                | 6.0%     | 0.0%       | 3.3%       | 0.9%       | 0.9%       | 0.0%     |\n",
            "| Qwen3-0.6B.. [FT ZS]                       | 99.4%    | 23.6%      | 30.7%      | 2.8%       | 43.4%      | 0.0%     |\n",
            "| ------------------------------------------ | -------- | ---------- | ---------- | ---------- | ---------- | -------- |\n",
            "| gemma-2-2b-it_B.. [Base ZS]                | 30.0%    | 5.7%       | 25.5%      | 36.8%      | 45.8%      | 0.0%     |\n",
            "| gemma-2-2b-it.. [FT ZS]                    | 62.3%    | 15.1%      | 9.4%       | 2.4%       | 29.2%      | 0.5%     |\n",
            "----------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}