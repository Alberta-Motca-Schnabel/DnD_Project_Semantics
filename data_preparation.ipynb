{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "KQAsJ_pgvZez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n",
        "\n",
        "from google.colab import drive\n",
        "from collections import Counter\n",
        "from docx import Document\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import sys\n",
        "import re\n",
        "import os\n",
        "import collections\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dpv0oXZyQWXN",
        "outputId": "7b76074c-467a-4d6c-9605-44a143718abb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (6.0.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/253.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m245.8/253.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API\n",
        "The output below demonstrates the constraints of using the open-source SRD ruleset:\n",
        "1.  **Backgrounds:** Likely restricts to a single entry (e.g., \"Acolyte\").\n",
        "2.  **Subclasses:** Typically includes only one subclass per class\n",
        "3.  **Races:** Limited\n",
        "\n",
        "Any dataset entry not matching these specific values is flagged by the strict validator"
      ],
      "metadata": {
        "id": "pQcvHLO8-Ob-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import textwrap\n",
        "\n",
        "API_BASE = \"https://www.dnd5eapi.co/api\"\n",
        "\n",
        "def fetch_all(endpoint):\n",
        "    \"\"\"Helper to fetch all results from an endpoint.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(f\"{API_BASE}/{endpoint}\", timeout=10)\n",
        "        response.raise_for_status()\n",
        "        return response.json()['results']\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {endpoint}: {e}\")\n",
        "        return []\n",
        "\n",
        "print(\"Fetching SRD reference data:\")\n",
        "\n",
        "# Classes\n",
        "classes = fetch_all(\"classes\")\n",
        "print(f\"\\n1. Classes ({len(classes)} found)\")\n",
        "print(f\"   Values: {', '.join([c['name'] for c in classes])}\")\n",
        "\n",
        "# Subclasses\n",
        "subclasses = fetch_all(\"subclasses\")\n",
        "print(f\"\\n2. Subclasses ({len(subclasses)} found)\")\n",
        "# Displaying first 5\n",
        "print(f\"   Sample: {', '.join([s['name'] for s in subclasses[:8]])}...\")\n",
        "\n",
        "# Races and Subraces\n",
        "races = fetch_all(\"races\")\n",
        "subraces = fetch_all(\"subraces\")\n",
        "print(f\"\\n3. Races ({len(races)}) & Subraces ({len(subraces)})\")\n",
        "\n",
        "# Map subraces to parents for display\n",
        "hierarchy = {r['name']: [] for r in races}\n",
        "for sub in subraces:\n",
        "    try:\n",
        "        det = requests.get(f\"{API_BASE}{sub['url']}\", timeout=5).json()\n",
        "        parent = det.get('race', {}).get('name')\n",
        "        if parent in hierarchy:\n",
        "            hierarchy[parent].append(sub['name'])\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "for race, subs in hierarchy.items():\n",
        "    if subs:\n",
        "        print(f\"   - {race}: {', '.join(subs)}\")\n",
        "    else:\n",
        "        print(f\"   - {race}\")\n",
        "\n",
        "# Backgrounds\n",
        "backgrounds = fetch_all(\"backgrounds\")\n",
        "print(f\"\\n4. Backgrounds ({len(backgrounds)} found)\")\n",
        "for b in backgrounds:\n",
        "    print(f\"   - Name: {b['name']} (Index: {b['index']})\")\n",
        "\n",
        "# Feats\n",
        "feats = fetch_all(\"feats\")\n",
        "print(f\"\\n5. Feats ({len(feats)} found)\")\n",
        "print(f\"   Sample: {', '.join([f['name'] for f in feats[:8]])}...\")\n",
        "\n",
        "# Skills\n",
        "skills = fetch_all(\"skills\")\n",
        "print(f\"\\n6. Skills ({len(skills)} found)\")\n",
        "print(f\"   Values: {', '.join([s['name'] for s in skills])}\")\n",
        "\n",
        "# Alignments\n",
        "alignments = fetch_all(\"alignments\")\n",
        "print(f\"\\n7. Alignments ({len(alignments)} found)\")\n",
        "print(f\"   Values: {', '.join([a['name'] for a in alignments])}\")\n",
        "\n",
        "# Spells\n",
        "try:\n",
        "    spell_count = requests.get(f\"{API_BASE}/spells?count=1\", timeout=10).json()['count']\n",
        "    print(f\"\\n8. Spells (Total SRD Spells: {spell_count})\")\n",
        "except:\n",
        "    print(\"\\n8. Spells (Error fetching count)\")\n",
        "\n",
        "# Weapon Categories\n",
        "try:\n",
        "    weapons = requests.get(f\"{API_BASE}/equipment-categories/weapon\", timeout=10).json()['equipment']\n",
        "    print(f\"\\n9. Standard Weapons ({len(weapons)} found)\")\n",
        "    print(f\"   Sample: {', '.join([w['name'] for w in weapons[:8]])}...\")\n",
        "except:\n",
        "    print(\"\\n9. Standard Weapons (Error fetching)\")"
      ],
      "metadata": {
        "id": "e1uPAUi9Qby8",
        "outputId": "0aad2690-7d50-4ec8-891b-27feba910c38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching SRD reference data:\n",
            "\n",
            "1. Classes (12 found)\n",
            "   Values: Barbarian, Bard, Cleric, Druid, Fighter, Monk, Paladin, Ranger, Rogue, Sorcerer, Warlock, Wizard\n",
            "\n",
            "2. Subclasses (12 found)\n",
            "   Sample: Berserker, Champion, Devotion, Draconic, Evocation, Fiend, Hunter, Land...\n",
            "\n",
            "3. Races (9) & Subraces (4)\n",
            "   - Dragonborn\n",
            "   - Dwarf\n",
            "   - Elf\n",
            "   - Gnome\n",
            "   - Half-Elf\n",
            "   - Half-Orc\n",
            "   - Halfling\n",
            "   - Human\n",
            "   - Tiefling\n",
            "\n",
            "4. Backgrounds (1 found)\n",
            "   - Name: Acolyte (Index: acolyte)\n",
            "\n",
            "5. Feats (1 found)\n",
            "   Sample: Grappler...\n",
            "\n",
            "6. Skills (18 found)\n",
            "   Values: Acrobatics, Animal Handling, Arcana, Athletics, Deception, History, Insight, Intimidation, Investigation, Medicine, Nature, Perception, Performance, Persuasion, Religion, Sleight of Hand, Stealth, Survival\n",
            "\n",
            "7. Alignments (9 found)\n",
            "   Values: Chaotic Evil, Chaotic Good, Chaotic Neutral, Lawful Evil, Lawful Good, Lawful Neutral, Neutral, Neutral Evil, Neutral Good\n",
            "\n",
            "8. Spells (Total SRD Spells: 319)\n",
            "\n",
            "9. Standard Weapons (67 found)\n",
            "   Sample: Club, Dagger, Greatclub, Handaxe, Javelin, Light hammer, Mace, Quarterstaff...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Strict API Validation\n",
        "\n",
        "Loads the raw dataset (`dnd_chars_unique.json`) and validates each character against the strict **ruleset** fetched from the API.\n",
        "\n",
        "**Logic:**\n",
        "1.  **Fetch & Map:** It downloads all valid options (Classes, Races, Backgrounds, etc.) from `dnd5eapi.co` and creates normalized lookup maps.\n",
        "2.  **Strict Filtering:** It iterates through every character sheet. If a character uses *any* content not present in the API it is flagged as **Invalid**.\n",
        "3.  **Split:**\n",
        "    * **Positive Dataset (`dataset_positive_cleaned.json`):** Characters that are 100% compliant with the open-source API rules.\n",
        "    * **Negative Dataset (`dataset_negative_raw.json`):** Characters containing non-SRD content, homebrew, or errors"
      ],
      "metadata": {
        "id": "gF4Zd23B_o8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import requests\n",
        "from google.colab import drive\n",
        "\n",
        "# CONFIGURATION\n",
        "print(\"Connecting to Google Drive\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/DnD_Project_Data\"\n",
        "INPUT_FILE = os.path.join(BASE_DIR, 'dnd_chars_unique.json')\n",
        "FILE_POS = os.path.join(BASE_DIR, 'dataset_positive_cleaned.json')\n",
        "FILE_NEG = os.path.join(BASE_DIR, 'dataset_negative_raw.json')\n",
        "\n",
        "API_BASE = \"https://www.dnd5eapi.co/api\"\n",
        "\n",
        "#  FUNCTIONS\n",
        "\n",
        "def super_normalize(text):\n",
        "    \"\"\"Removes all non-alphanumeric characters for safe comparison.\"\"\"\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    return re.sub(r'[^a-z0-9]', '', text.lower())\n",
        "\n",
        "def get_scalar(data, key, default=None):\n",
        "    \"\"\"Extracts a single string value from a field that might be a list or dict.\"\"\"\n",
        "    if not isinstance(data, dict): return default\n",
        "    val = data.get(key)\n",
        "    if isinstance(val, list) and len(val) > 0: val = val[0]\n",
        "    return str(val).strip() if val is not None else default\n",
        "\n",
        "def fetch_resource_map(endpoint):\n",
        "    \"\"\"Fetches a resource list from the API and returns a normalization map.\"\"\"\n",
        "    try:\n",
        "        resp = requests.get(f\"{API_BASE}/{endpoint}\", timeout=20)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "        if 'results' in data:\n",
        "            # Returns: {'normalized_name': 'Official Name'}\n",
        "            return {super_normalize(item['name']): item['name'] for item in data['results']}\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {endpoint}: {e}\")\n",
        "    return {}\n",
        "\n",
        "#  BUILD VALIDATION MAPS\n",
        "print(\"Fetching SRD Rules and building validation maps...\")\n",
        "\n",
        "resources = {\n",
        "    'classes': 'classes',\n",
        "    'subclasses': 'subclasses',\n",
        "    'races': 'races',\n",
        "    'backgrounds': 'backgrounds',\n",
        "    'feats': 'feats',\n",
        "    'skills': 'skills',\n",
        "    'alignments': 'alignments'\n",
        "}\n",
        "\n",
        "master_maps = {}\n",
        "for key, url in resources.items():\n",
        "    master_maps[key] = fetch_resource_map(url)\n",
        "    print(f\"   - {key.capitalize()}: {len(master_maps[key])} valid entries loaded.\")\n",
        "\n",
        "print(\"Validation maps ready.\")\n",
        "\n",
        "# VALIDATION LOOP\n",
        "print(f\" Processing dataset from: {INPUT_FILE}\")\n",
        "\n",
        "dataset_pos = []\n",
        "dataset_neg = []\n",
        "\n",
        "try:\n",
        "    with open(INPUT_FILE, 'r') as f:\n",
        "        raw_data = json.load(f)\n",
        "\n",
        "    for cid, data in raw_data.items():\n",
        "        try:\n",
        "            clean_entry = {\"id\": cid}\n",
        "\n",
        "            # RACE\n",
        "            rc_d = data.get('race', {})\n",
        "            raw_rc = get_scalar(rc_d, 'processedRace') or get_scalar(rc_d, 'race')\n",
        "            norm_rc = super_normalize(raw_rc)\n",
        "\n",
        "            if norm_rc not in master_maps['races']:\n",
        "                raise ValueError(f\"Invalid Race: {raw_rc}\")\n",
        "            clean_entry['race'] = master_maps['races'][norm_rc]\n",
        "\n",
        "            # CLASS\n",
        "            c_dict = data.get('class', {})\n",
        "            if not c_dict: raise ValueError(\"Missing Class\")\n",
        "            raw_c = list(c_dict.keys())[0]\n",
        "            norm_c = super_normalize(raw_c)\n",
        "\n",
        "            if norm_c not in master_maps['classes']:\n",
        "                raise ValueError(f\"Invalid Class: {raw_c}\")\n",
        "            clean_entry['class'] = master_maps['classes'][norm_c]\n",
        "\n",
        "            # BACKGROUND\n",
        "            bg = get_scalar(data, 'background')\n",
        "            norm_bg = super_normalize(bg)\n",
        "\n",
        "            if norm_bg not in master_maps['backgrounds']:\n",
        "                raise ValueError(f\"Invalid Background: {bg}\")\n",
        "            clean_entry['background'] = master_maps['backgrounds'][norm_bg]\n",
        "\n",
        "            # SUBCLASS\n",
        "            raw_sc = get_scalar(c_dict[raw_c], 'subclass')\n",
        "            norm_sc = super_normalize(raw_sc)\n",
        "            if norm_sc in master_maps['subclasses']:\n",
        "                clean_entry['subclass'] = master_maps['subclasses'][norm_sc]\n",
        "            else:\n",
        "                clean_entry['subclass'] = None\n",
        "\n",
        "            # ALIGNMENT\n",
        "            al_dict = data.get('alignment', {})\n",
        "            raw_al = get_scalar(al_dict, 'processedAlignment') or get_scalar(al_dict, 'alignment')\n",
        "            norm_al = super_normalize(raw_al)\n",
        "            clean_entry['alignment'] = master_maps['alignments'].get(norm_al)\n",
        "\n",
        "            # SKILLS & FEATS: keep only valid items, discarding invalid ones without failing the whole sheet\n",
        "            clean_entry['skills'] = [\n",
        "                master_maps['skills'][super_normalize(s)]\n",
        "                for s in data.get('skills', [])\n",
        "                if super_normalize(s) in master_maps['skills']\n",
        "            ]\n",
        "\n",
        "            clean_entry['feats'] = [\n",
        "                master_maps['feats'][super_normalize(f)]\n",
        "                for f in data.get('feats', [])\n",
        "                if super_normalize(f) in master_maps['feats']\n",
        "            ]\n",
        "\n",
        "            # Add to positive list\n",
        "            dataset_pos.append(clean_entry)\n",
        "\n",
        "        except ValueError as e:\n",
        "            # Add to negative list\n",
        "            dataset_neg.append({\"id\": cid, \"error\": str(e)})\n",
        "\n",
        "    # SAVE RESULTS\n",
        "    print(f\"VALIDATION SUMMARY\")\n",
        "    print(f\"STRICTLY VALID (SRD): {len(dataset_pos)}\")\n",
        "    print(f\"REJECTED (Non-SRD):   {len(dataset_neg)}\")\n",
        "\n",
        "    with open(FILE_POS, 'w') as f: json.dump(dataset_pos, f, indent=2)\n",
        "    with open(FILE_NEG, 'w') as f: json.dump(dataset_neg, f, indent=2)\n",
        "    print(f\"\\nFiles saved to: {BASE_DIR}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Input file not found at {INPUT_FILE}\")\n",
        "except Exception as e:\n",
        "    print(f\"Unexpected Error: {e}\")"
      ],
      "metadata": {
        "id": "x-YH5LjmwGcg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fa80e52-74a3-4677-f185-e48394bc8774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to Google Drive\n",
            "Mounted at /content/drive\n",
            "Fetching SRD Rules and building validation maps...\n",
            "   - Classes: 12 valid entries loaded.\n",
            "   - Subclasses: 12 valid entries loaded.\n",
            "   - Races: 9 valid entries loaded.\n",
            "   - Backgrounds: 1 valid entries loaded.\n",
            "   - Feats: 1 valid entries loaded.\n",
            "   - Skills: 18 valid entries loaded.\n",
            "   - Alignments: 9 valid entries loaded.\n",
            "Validation maps ready.\n",
            " Processing dataset from: /content/drive/MyDrive/DnD_Project_Data/dnd_chars_unique.json\n",
            "VALIDATION SUMMARY\n",
            "STRICTLY VALID (SRD): 413\n",
            "REJECTED (Non-SRD):   7533\n",
            "\n",
            "Files saved to: /content/drive/MyDrive/DnD_Project_Data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Gap Analysis\n",
        "\n",
        "This step analyzes the **Negative Dataset** (items rejected by the strict SRD validator) to identify high-frequency content.\n",
        "\n",
        "**Purpose:**\n",
        "The validator rejects valid official content. This analysis quantifies exactly what is missing.\n",
        "\n",
        "**Process:**\n",
        "1.  **Cross-Reference:** It matches the IDs from the negative dataset back to the raw input to retrieve the full character data.\n",
        "2.  **Frequency Counting:** It counts every occurrence of a rejected Race, Class, Background, Feat, or Spell.\n",
        "3.  **Reporting:** It generates Markdown reports (e.g., `Gap_Backgrounds.md`) listing the missing elements.\n",
        "\n",
        "**Output:**\n",
        "These reports guide the creation of the **Manual Whitelist** used in the next integration step."
      ],
      "metadata": {
        "id": "eGUcUA73j7jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "REPORT_DIR = os.path.join(BASE_DIR, \"analysis_reports_gap\")\n",
        "os.makedirs(REPORT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Loading data for analysis\")\n",
        "\n",
        "# Load Negative Dataset\n",
        "with open(FILE_NEG, 'r') as f:\n",
        "    neg_data = json.load(f)\n",
        "\n",
        "# Load Full Raw Dataset\n",
        "print(f\"   - Reading raw source: {INPUT_FILE}\")\n",
        "with open(INPUT_FILE, 'r') as f:\n",
        "    full_raw_data = json.load(f)\n",
        "\n",
        "# Initialize counters for each category\n",
        "gap_counters = {\n",
        "    \"Races\": collections.Counter(),\n",
        "    \"Classes\": collections.Counter(),\n",
        "    \"Backgrounds\": collections.Counter(),\n",
        "    \"Feats\": collections.Counter(),\n",
        "    \"Skills\": collections.Counter(),\n",
        "    \"Alignments\": collections.Counter(),\n",
        "    \"Spells\": collections.Counter(),\n",
        "}\n",
        "\n",
        "print(\"Analyzing rejection\")\n",
        "\n",
        "for entry in neg_data:\n",
        "    cid = entry['id']\n",
        "\n",
        "    if cid not in full_raw_data:\n",
        "        continue\n",
        "\n",
        "    orig = full_raw_data[cid]\n",
        "\n",
        "    # RACE GAP\n",
        "    rc_d = orig.get('race', {})\n",
        "    # Handle mixed formats\n",
        "    if isinstance(rc_d, dict):\n",
        "        raw_rc = get_scalar(rc_d, 'processedRace') or get_scalar(rc_d, 'race')\n",
        "    else:\n",
        "        raw_rc = str(rc_d)\n",
        "\n",
        "    if raw_rc:\n",
        "        norm_rc = super_normalize(raw_rc)\n",
        "        if norm_rc not in master_maps['races']:\n",
        "            gap_counters[\"Races\"][raw_rc] += 1\n",
        "\n",
        "    # CLASS & SUBCLASS GAP\n",
        "    c_d = orig.get('class', {})\n",
        "    if c_d and isinstance(c_d, dict):\n",
        "        raw_c = list(c_d.keys())[0]\n",
        "        norm_c = super_normalize(raw_c)\n",
        "\n",
        "        # Check Base Class\n",
        "        if norm_c not in master_maps['classes']:\n",
        "            gap_counters[\"Classes\"][raw_c] += 1\n",
        "\n",
        "        # Check Subclass\n",
        "        raw_sc = get_scalar(c_d[raw_c], 'subclass')\n",
        "        if raw_sc:\n",
        "            norm_sc = super_normalize(raw_sc)\n",
        "            if norm_sc not in master_maps['subclasses']:\n",
        "                 gap_counters[\"Classes\"][f\"Subclass: {raw_sc}\"] += 1\n",
        "\n",
        "    # BACKGROUND GAP\n",
        "    bg = get_scalar(orig, 'background')\n",
        "    if bg:\n",
        "        norm_bg = super_normalize(bg)\n",
        "        if norm_bg not in master_maps['backgrounds']:\n",
        "            gap_counters[\"Backgrounds\"][bg] += 1\n",
        "\n",
        "    # FEATS GAP\n",
        "    for ft in orig.get('feats', []):\n",
        "        if super_normalize(ft) not in master_maps['feats']:\n",
        "            gap_counters[\"Feats\"][ft] += 1\n",
        "\n",
        "    # SKILLS GAP\n",
        "    for sk in orig.get('skills', []):\n",
        "        if super_normalize(sk) not in master_maps['skills']:\n",
        "            gap_counters[\"Skills\"][sk] += 1\n",
        "\n",
        "    # ALIGNMENT GAP\n",
        "    al_d = orig.get('alignment', {})\n",
        "    raw_al = get_scalar(al_d, 'processedAlignment') or get_scalar(al_d, 'alignment')\n",
        "    if raw_al and super_normalize(raw_al) not in master_maps['alignments']:\n",
        "        gap_counters[\"Alignments\"][raw_al] += 1\n",
        "\n",
        "    # SPELLS GAP\n",
        "    if 'spells' in master_maps and master_maps['spells']:\n",
        "        spells_raw = orig.get('spells', {})\n",
        "        iterator = spells_raw.values() if isinstance(spells_raw, dict) else spells_raw\n",
        "        for v in iterator:\n",
        "            s_name = v if isinstance(v, str) else (get_scalar(v, 'processedSpell') or get_scalar(v, 'spell'))\n",
        "            if s_name and super_normalize(s_name) not in master_maps['spells']:\n",
        "                gap_counters[\"Spells\"][s_name] += 1\n",
        "\n",
        "# reports\n",
        "print(f\"\\n Writing Reports to: {REPORT_DIR}\")\n",
        "\n",
        "for category, counter in gap_counters.items():\n",
        "    if not counter: continue\n",
        "\n",
        "    filepath = os.path.join(REPORT_DIR, f\"Gap_{category}.md\")\n",
        "\n",
        "    with open(filepath, 'w') as f:\n",
        "        f.write(f\"# Gap Analysis: {category}\\n\\n\")\n",
        "        f.write(f\"| Missing Value | Frequency |\\n\")\n",
        "        f.write(f\"| :--- | :--- |\\n\")\n",
        "\n",
        "        for val, count in counter.most_common():\n",
        "            safe_val = str(val).replace(\"|\", \"-\").replace(\"\\n\", \" \")\n",
        "            f.write(f\"| {safe_val} | {count} |\\n\")\n",
        "\n",
        "    print(f\" Created: Gap_{category}.md\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BiBGQqXqpqQ",
        "outputId": "b91d4a1f-33ea-48fb-e909-c9f39aa991bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data for analysis\n",
            "   - Reading raw source: /content/drive/MyDrive/DnD_Project_Data/dnd_chars_unique.json\n",
            "Analyzing rejection\n",
            "\n",
            " Writing Reports to: /content/drive/MyDrive/DnD_Project_Data/analysis_reports_gap\n",
            " Created: Gap_Races.md\n",
            " Created: Gap_Classes.md\n",
            " Created: Gap_Backgrounds.md\n",
            " Created: Gap_Feats.md\n",
            " Created: Gap_Alignments.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rules Enforcement\n",
        "\n",
        "This step merges the API data with manual whitelists to create the final dataset. It enforces the **mathematical and logical rules of D&D 5th Edition** and  **repairs** valuable training samples.\n",
        "\n",
        "**Key Validations & Features:**\n",
        "\n",
        "1.  **Hybrid Knowledge Base (API + Manual Parsing):**\n",
        "    * *Source:* `dnd5eapi.co` (SRD) + `DnD_Integrated.docx` (Manual).\n",
        "    * *Logic:* The system parses specific data from the manual file (e.g., reading `\"Toll the Dead - 0\"` to assign Level 0). This allows strict validation even for non-SRD content.\n",
        "\n",
        "2.  **Universal Spell Slot Validation:**\n",
        "    * *Rule:* Characters cannot cast spells higher than their level permits.\n",
        "    * *Math:* Implements precise progression formulas for **Full Casters** ($\\lceil Lvl/2 \\rceil$), **Half-Casters** (Paladin/Ranger), and **Pact Magic** (Warlock caps at Lv5). Spells exceeding the max slot are pruned.\n",
        "\n",
        "3.  **Mandatory Spellcasting & Repair:**\n",
        "    * *Problem:* High-level casters (e.g., Wizard Lv10) with empty spell lists caused by parsing errors or incomplete inputs.\n",
        "    * *Solution:* Instead of discarding these valid characters, select 2 random valid spells (Lv0 or Lv1) from a class-specific pool. This preserves the data structure\n",
        "\n",
        "4.  **Subclass Logic & Unlock Levels:**\n",
        "    * *Rule:* Subclasses are validated based on unlock levels (Lv1 for Sorcerers/Clerics, Lv3 for others).\n",
        "    * *Edge Cases:* Specifically handles \"Third-Casters\" (Arcane Trickster/Eldritch Knight), enforcing spell requirements only from Level 3 onwards.\n",
        "\n",
        "5.  **Stat & HP Integrity:**\n",
        "    * *Stats:* Enforces valid ranges (typically 1-20, max 30 for epic items).\n",
        "    * *Hit Points:* Validates HP against the class Hit Die (e.g., Wizard d6 vs Barbarian d12) to ensure mathematical consistency.\n",
        "\n",
        "6.  **Alignment Normalization:**\n",
        "    * *Logic:* Maps chaotic inputs (e.g., \"CG\", \"chaotic-good\") to the 9 canonical alignment strings."
      ],
      "metadata": {
        "id": "aFfgX-MRGc64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "import requests\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "import collections\n",
        "import random\n",
        "from google.colab import drive\n",
        "try:\n",
        "    from docx import Document\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "print(\"Connecting to Google Drive\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/DnD_Project_Data\"\n",
        "INPUT_FILE = os.path.join(BASE_DIR, 'dnd_chars_unique.json')\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, \"dataset_integrated\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "FILE_POS_INT = os.path.join(OUTPUT_DIR, 'dataset_integrated_positive.json')\n",
        "FILE_NEG_INT = os.path.join(OUTPUT_DIR, 'dataset_integrated_negative.json')\n",
        "WHITELIST_DOC_PATH = os.path.join(BASE_DIR, \"analysis_reports_gap\", \"DnD_Integrated.docx\")\n",
        "\n",
        "API_BASE = \"https://www.dnd5eapi.co/api\"\n",
        "GRAPHQL_URL = \"https://www.dnd5eapi.co/graphql\"\n",
        "\n",
        "# 5E RULES CONSTANTS\n",
        "REQUIRED_STATS = {'str', 'dex', 'con', 'int', 'wis', 'cha'}\n",
        "MAX_STAT_VALUE = 30\n",
        "MAX_STRING_LEN = 60\n",
        "MAX_SKILLS_COUNT = 18\n",
        "MAX_FEATS_COUNT = 10\n",
        "\n",
        "SUBCLASS_UNLOCK_LEVELS = {\n",
        "    \"cleric\": 1, \"sorcerer\": 1, \"warlock\": 1,\n",
        "    \"druid\": 2, \"wizard\": 2,\n",
        "    \"default\": 3\n",
        "}\n",
        "\n",
        "SPELLCASTING_REQUIRED_AT = {\n",
        "    \"bard\": 1, \"cleric\": 1, \"druid\": 1, \"sorcerer\": 1, \"warlock\": 1, \"wizard\": 1,\n",
        "    \"artificer\": 1,\n",
        "    \"paladin\": 2, \"ranger\": 2\n",
        "}\n",
        "\n",
        "# DEFAULT SPELL POOLS\n",
        "# safe spells (Lv 0 & Lv 1)\n",
        "DEFAULT_SPELL_POOLS = {\n",
        "    \"bard\": [\"Vicious Mockery\", \"Prestidigitation\", \"Minor Illusion\", \"Healing Word\", \"Detect Magic\", \"Thunderwave\"],\n",
        "    \"cleric\": [\"Thaumaturgy\", \"Light\", \"Sacred Flame\", \"Guidance\", \"Bless\", \"Cure Wounds\", \"Healing Word\", \"Shield of Faith\"],\n",
        "    \"druid\": [\"Druidcraft\", \"Produce Flame\", \"Shillelagh\", \"Thorn Whip\", \"Entangle\", \"Goodberry\", \"Thunderwave\"],\n",
        "    \"sorcerer\": [\"Light\", \"Prestidigitation\", \"Fire Bolt\", \"Ray of Frost\", \"Shield\", \"Magic Missile\", \"Burning Hands\"],\n",
        "    \"warlock\": [\"Eldritch Blast\", \"Prestidigitation\", \"Minor Illusion\", \"Hex\", \"Armor of Agathys\", \"Hellish Rebuke\"],\n",
        "    \"wizard\": [\"Prestidigitation\", \"Light\", \"Mage Hand\", \"Fire Bolt\", \"Magic Missile\", \"Shield\", \"Mage Armor\", \"Sleep\"],\n",
        "    \"artificer\": [\"Prestidigitation\", \"Mending\", \"Guidance\", \"Cure Wounds\", \"Detect Magic\", \"Grease\"],\n",
        "\n",
        "    # Half Casters (Lv 1)\n",
        "    \"paladin\": [\"Bless\", \"Cure Wounds\", \"Divine Favor\", \"Shield of Faith\", \"Command\"],\n",
        "    \"ranger\": [\"Cure Wounds\", \"Fog Cloud\", \"Hunter's Mark\", \"Detect Magic\", \"Goodberry\"],\n",
        "\n",
        "    # Subclass Casters\n",
        "    \"fighter\": [\"Blade Ward\", \"Light\", \"Shield\", \"Magic Missile\"],\n",
        "    \"rogue\": [\"Mage Hand\", \"Prestidigitation\", \"Minor Illusion\", \"Sleep\"]\n",
        "}\n",
        "\n",
        "# Alignment Map\n",
        "ALIGNMENT_MAP = {\n",
        "    \"lg\": \"Lawful Good\", \"lawfulgood\": \"Lawful Good\",\n",
        "    \"ng\": \"Neutral Good\", \"neutralgood\": \"Neutral Good\", \"good\": \"Neutral Good\",\n",
        "    \"cg\": \"Chaotic Good\", \"chaoticgood\": \"Chaotic Good\",\n",
        "    \"ln\": \"Lawful Neutral\", \"lawfulneutral\": \"Lawful Neutral\",\n",
        "    \"n\": \"True Neutral\", \"neutral\": \"True Neutral\", \"trueneutral\": \"True Neutral\", \"tn\": \"True Neutral\",\n",
        "    \"cn\": \"Chaotic Neutral\", \"chaoticneutral\": \"Chaotic Neutral\",\n",
        "    \"le\": \"Lawful Evil\", \"lawfulevil\": \"Lawful Evil\",\n",
        "    \"ne\": \"Neutral Evil\", \"neutralevil\": \"Neutral Evil\", \"evil\": \"Neutral Evil\",\n",
        "    \"ce\": \"Chaotic Evil\", \"chaoticevil\": \"Chaotic Evil\",\n",
        "    \"unaligned\": \"Unaligned\", \"any\": \"Any Alignment\"\n",
        "}\n",
        "\n",
        "# FUNCTIONS\n",
        "\n",
        "def super_normalize(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    return re.sub(r'[^a-z0-9]', '', text.lower())\n",
        "\n",
        "def extract_scalar(val):\n",
        "    if isinstance(val, list): return val[0] if len(val) > 0 else None\n",
        "    return val\n",
        "\n",
        "def calculate_real_level(class_obj):\n",
        "    total = 0\n",
        "    main_class_name = \"Unknown\"\n",
        "    max_lvl = 0\n",
        "\n",
        "    if isinstance(class_obj, dict):\n",
        "        for c_name, c_data in class_obj.items():\n",
        "            lvl = 0\n",
        "            if isinstance(c_data, dict): lvl = int(extract_scalar(c_data.get('level', 0)) or 0)\n",
        "            elif isinstance(c_data, list): lvl = int(c_data[0]) if c_data else 0\n",
        "            else:\n",
        "                try: lvl = int(c_data)\n",
        "                except: pass\n",
        "\n",
        "            total += lvl\n",
        "            if lvl > max_lvl:\n",
        "                max_lvl = lvl\n",
        "                main_class_name = c_name\n",
        "\n",
        "    return (total if total > 0 else 1), main_class_name\n",
        "\n",
        "def validate_stats(attributes_dict):\n",
        "    if not isinstance(attributes_dict, dict): raise ValueError(\"Stats Missing\")\n",
        "    clean = {}\n",
        "    found_keys = 0\n",
        "    for k, v in attributes_dict.items():\n",
        "        norm_k = k.lower()[:3]\n",
        "        if norm_k in REQUIRED_STATS:\n",
        "            try:\n",
        "                val_int = int(extract_scalar(v))\n",
        "                if val_int <= 0: raise ValueError # Min 1\n",
        "                if val_int > MAX_STAT_VALUE: raise ValueError(f\"Stat > {MAX_STAT_VALUE}\")\n",
        "                clean[norm_k] = val_int\n",
        "                found_keys += 1\n",
        "            except: raise ValueError(f\"Invalid value for {k}\")\n",
        "\n",
        "    if found_keys < 6: raise ValueError(f\"Missing one or more attributes\")\n",
        "    return clean\n",
        "\n",
        "def get_max_spell_level(char_level, class_name):\n",
        "    cls = class_name.lower()\n",
        "    if \"warlock\" in cls:\n",
        "        if char_level < 1: return 0\n",
        "        return min(5, math.ceil(char_level / 2.0))\n",
        "    if any(x in cls for x in [\"paladin\", \"ranger\"]):\n",
        "        return math.ceil(char_level / 2.0) if char_level >= 2 else 0\n",
        "    return math.ceil(char_level / 2.0)\n",
        "\n",
        "# WHITELISTS CONTAINERS\n",
        "whitelists = {\n",
        "    \"background\": set(), \"feats\": set(), \"races\": set(),\n",
        "    \"subclasses\": set(), \"skills\": set(), \"spells\": set(), \"classes\": set()\n",
        "}\n",
        "\n",
        "manual_spells_lvl = {}\n",
        "\n",
        "# --- 1. LOAD MANUAL WHITELISTS (DOCX) ---\n",
        "if os.path.exists(WHITELIST_DOC_PATH):\n",
        "    print(f\" - Integrating Manual Whitelist: {os.path.basename(WHITELIST_DOC_PATH)}\")\n",
        "    try:\n",
        "        doc = Document(WHITELIST_DOC_PATH)\n",
        "        current_cat = None\n",
        "        for p in doc.paragraphs:\n",
        "            line = p.text.strip()\n",
        "            if not line: continue\n",
        "            low = line.lower()\n",
        "\n",
        "            if \"background\" in low and \":\" in low: current_cat = \"background\"\n",
        "            elif \"feat\" in low and \":\" in low: current_cat = \"feats\"\n",
        "            elif \"race\" in low and \":\" in low: current_cat = \"races\"\n",
        "            elif \"subclass\" in low and \":\" in low: current_cat = \"subclasses\"\n",
        "            elif \"skill\" in low and \":\" in low: current_cat = \"skills\"\n",
        "            elif \"class\" in low and \":\" in low: current_cat = \"classes\"\n",
        "            elif \"spell\" in low and \":\" in low: current_cat = \"spells\"\n",
        "\n",
        "            elif current_cat:\n",
        "                val = line.split(\":\")[-1].strip() if \":\" in line else line\n",
        "                if current_cat == \"spells\":\n",
        "                    parts = val.rsplit('-', 1)\n",
        "                    if len(parts) == 2 and parts[1].strip().isdigit():\n",
        "                        s_name = parts[0].strip()\n",
        "                        s_lvl = int(parts[1].strip())\n",
        "                        norm = super_normalize(s_name)\n",
        "                        whitelists['spells'].add(norm)\n",
        "                        manual_spells_lvl[norm] = s_lvl\n",
        "                    else:\n",
        "                        norm = super_normalize(val)\n",
        "                        whitelists['spells'].add(norm)\n",
        "                else:\n",
        "                    whitelists[current_cat].add(super_normalize(val))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Could not read manual whitelist ({e})\")\n",
        "\n",
        "print(f\"   Manual Spells with Levels Loaded: {len(manual_spells_lvl)}\")\n",
        "\n",
        "# API DATA\n",
        "print(\" - Fetching API Data...\")\n",
        "session = requests.Session()\n",
        "api_classes_hd = {}\n",
        "api_spells_lvl = {}\n",
        "\n",
        "try:\n",
        "    r = session.get(f\"{API_BASE}/classes\", timeout=10).json()['results']\n",
        "    for c in r:\n",
        "        detail = session.get(f\"{API_BASE}/classes/{c['index']}\").json()\n",
        "        norm_name = super_normalize(c['name'])\n",
        "        api_classes_hd[norm_name] = detail.get('hit_die', 8)\n",
        "        whitelists['classes'].add(norm_name)\n",
        "\n",
        "    print(\"Fetching Spell Index (via GraphQL)\")\n",
        "    query = \"\"\"\n",
        "    query {\n",
        "      spells(limit: 2000) {\n",
        "        name\n",
        "        level\n",
        "      }\n",
        "    }\n",
        "    \"\"\"\n",
        "    r_spells_response = session.post(GRAPHQL_URL, json={'query': query}, timeout=30)\n",
        "    r_spells_response.raise_for_status()\n",
        "    all_spells = r_spells_response.json().get('data', {}).get('spells', [])\n",
        "\n",
        "    for s in all_spells:\n",
        "        api_spells_lvl[super_normalize(s['name'])] = s['level']\n",
        "\n",
        "    print(f\"        Indexed {len(api_spells_lvl)} spells via GraphQL.\")\n",
        "\n",
        "    def merge_api_list(endpoint, category_key):\n",
        "        items = session.get(f\"{API_BASE}/{endpoint}\").json()['results']\n",
        "        for i in items:\n",
        "            whitelists[category_key].add(super_normalize(i['name']))\n",
        "\n",
        "    merge_api_list(\"subclasses\", \"subclasses\")\n",
        "    merge_api_list(\"races\", \"races\")\n",
        "    merge_api_list(\"backgrounds\", \"background\")\n",
        "    merge_api_list(\"feats\", \"feats\")\n",
        "    merge_api_list(\"skills\", \"skills\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"API Error: {e}\")\n",
        "\n",
        "print(\"Validation Logic Ready.\")\n",
        "\n",
        "# INTEGRATION LOOP\n",
        "print(f\"Validating & Integrating Data from: {INPUT_FILE}\")\n",
        "\n",
        "dataset_pos = []\n",
        "dataset_neg = []\n",
        "stats_rejected = collections.Counter()\n",
        "augmented_count = 0\n",
        "\n",
        "with open(INPUT_FILE, 'r') as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "for cid, d in raw_data.items():\n",
        "    race_obj = d.get('race', {})\n",
        "    raw_race = extract_scalar(race_obj.get('processedRace')) or extract_scalar(race_obj.get('race')) if isinstance(race_obj, dict) else str(race_obj)\n",
        "    raw_subrace = extract_scalar(race_obj.get('subrace')) if isinstance(race_obj, dict) else None\n",
        "    raw_bg = extract_scalar(d.get('background')) or \"Unknown\"\n",
        "\n",
        "    lvl_val, class_name_val = calculate_real_level(d.get('class'))\n",
        "\n",
        "    raw_subclass = None\n",
        "    if isinstance(d.get('class'), dict) and class_name_val in d['class']:\n",
        "        raw_subclass = extract_scalar(d['class'][class_name_val].get('subclass'))\n",
        "\n",
        "    try:\n",
        "        c_norm = super_normalize(class_name_val)\n",
        "\n",
        "        if len(raw_race) > MAX_STRING_LEN: raise ValueError(\"Race Name too long\")\n",
        "\n",
        "        if super_normalize(raw_race) not in whitelists['races']:\n",
        "            stats_rejected[\"INVALID_RACE\"] += 1\n",
        "            raise ValueError(f\"Invalid/Homebrew Race: {raw_race}\")\n",
        "\n",
        "        if super_normalize(raw_bg) not in whitelists['background']:\n",
        "            stats_rejected[\"INVALID_BACKGROUND\"] += 1\n",
        "            raise ValueError(f\"Invalid/Homebrew Background: {raw_bg}\")\n",
        "\n",
        "        try:\n",
        "            clean_stats = validate_stats(d.get('attributes') or d.get('stats') or {})\n",
        "        except ValueError as e:\n",
        "            stats_rejected[\"INVALID_STATS\"] += 1\n",
        "            raise ValueError(str(e))\n",
        "\n",
        "        hp_val = int(extract_scalar(d.get('HP')) or 0)\n",
        "\n",
        "        if c_norm not in api_classes_hd:\n",
        "            stats_rejected[\"UNKNOWN_CLASS\"] += 1\n",
        "            raise ValueError(f\"Unknown Class (No Hit Die info): {class_name_val}\")\n",
        "\n",
        "        hd_val = api_classes_hd[c_norm]\n",
        "        max_possible_hp = (hd_val + 7) * lvl_val * 2\n",
        "        if hp_val <= 0 or hp_val > max_possible_hp:\n",
        "            stats_rejected[\"IMPOSSIBLE_HP\"] += 1\n",
        "            raise ValueError(f\"HP {hp_val} out of valid range\")\n",
        "\n",
        "        subclass_val = None\n",
        "        unlock_lvl = SUBCLASS_UNLOCK_LEVELS.get(c_norm, SUBCLASS_UNLOCK_LEVELS[\"default\"])\n",
        "\n",
        "        if lvl_val >= unlock_lvl:\n",
        "            if not raw_subclass:\n",
        "                stats_rejected[\"MISSING_SUBCLASS\"] += 1\n",
        "                raise ValueError(\"Missing Required Subclass\")\n",
        "\n",
        "            if super_normalize(raw_subclass) not in whitelists['subclasses']:\n",
        "                stats_rejected[\"INVALID_SUBCLASS\"] += 1\n",
        "                raise ValueError(f\"Invalid Subclass: {raw_subclass}\")\n",
        "            subclass_val = raw_subclass\n",
        "        else:\n",
        "            subclass_val = None\n",
        "\n",
        "        raw_al_obj = d.get('alignment', {})\n",
        "        raw_al = extract_scalar(raw_al_obj.get('processedAlignment')) or extract_scalar(raw_al_obj.get('alignment')) if isinstance(raw_al_obj, dict) else extract_scalar(raw_al_obj)\n",
        "        al_val = ALIGNMENT_MAP.get(super_normalize(raw_al))\n",
        "\n",
        "        skills = [s for s in (d.get('skills') or []) if super_normalize(s) in whitelists['skills']]\n",
        "        if len(skills) > MAX_SKILLS_COUNT: raise ValueError(\"Too many skills\")\n",
        "\n",
        "        feats = [f for f in (d.get('feats') or []) if super_normalize(f) in whitelists['feats']]\n",
        "\n",
        "        is_variant_human = \"variant\" in str(raw_race).lower()\n",
        "        if lvl_val < 4 and len(feats) > 0 and not is_variant_human:\n",
        "            feats = []\n",
        "\n",
        "        # SPELL CHECK\n",
        "        spells_raw = d.get('spells', [])\n",
        "        clean_spells = []\n",
        "        spell_iter = spells_raw if isinstance(spells_raw, list) else [item for sublist in spells_raw.values() for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
        "\n",
        "        max_slot = get_max_spell_level(lvl_val, c_norm)\n",
        "\n",
        "        for s in spell_iter:\n",
        "            s_str = str(s) if not isinstance(s, dict) else (s.get('processedSpell') or s.get('spell'))\n",
        "            if not s_str: continue\n",
        "\n",
        "            s_norm = super_normalize(s_str)\n",
        "            spell_lvl = None\n",
        "\n",
        "            if s_norm in api_spells_lvl:\n",
        "                spell_lvl = api_spells_lvl[s_norm]\n",
        "            elif s_norm in manual_spells_lvl:\n",
        "                spell_lvl = manual_spells_lvl[s_norm]\n",
        "            elif s_norm in whitelists['spells']:\n",
        "                clean_spells.append(s_str)\n",
        "                continue\n",
        "\n",
        "            if spell_lvl is not None:\n",
        "                if spell_lvl == 0 or spell_lvl <= max_slot:\n",
        "                    clean_spells.append(s_str)\n",
        "\n",
        "        # REPAIR\n",
        "        required_lvl_for_spells = SPELLCASTING_REQUIRED_AT.get(c_norm)\n",
        "\n",
        "        sub_norm = super_normalize(subclass_val) if subclass_val else \"\"\n",
        "        if \"arcanetrickster\" in sub_norm or \"eldritchknight\" in sub_norm:\n",
        "             required_lvl_for_spells = 3\n",
        "\n",
        "        if required_lvl_for_spells is not None:\n",
        "            if lvl_val >= required_lvl_for_spells and not clean_spells:\n",
        "\n",
        "                pool = DEFAULT_SPELL_POOLS.get(c_norm)\n",
        "                if pool:\n",
        "                    # take 2 spell random\n",
        "                    num_to_pick = min(len(pool), 2)\n",
        "                    clean_spells = random.sample(pool, num_to_pick)\n",
        "                    augmented_count += 1\n",
        "                else:\n",
        "                    stats_rejected[\"MISSING_REQUIRED_SPELLS\"] += 1\n",
        "                    raise ValueError(f\"Class {class_name_val} requires spells, but none valid/default found.\")\n",
        "\n",
        "        raw_weapons = d.get('weapons', [])\n",
        "        weapons = [str(w) for w in raw_weapons if w]\n",
        "\n",
        "        dataset_pos.append({\n",
        "            \"race\": str(raw_race),\n",
        "            \"subrace\": raw_subrace,\n",
        "            \"class\": str(class_name_val),\n",
        "            \"subclass\": subclass_val,\n",
        "            \"level\": int(lvl_val),\n",
        "            \"background\": str(raw_bg),\n",
        "            \"stats\": clean_stats,\n",
        "            \"alignment\": al_val,\n",
        "            \"hp\": int(hp_val),\n",
        "            \"ac\": int(extract_scalar(d.get('AC'))) if d.get('AC') is not None else None,\n",
        "            \"skills\": skills,\n",
        "            \"spells\": clean_spells,\n",
        "            \"feats\": feats,\n",
        "            \"weapons\": weapons,\n",
        "        })\n",
        "\n",
        "    except ValueError as e:\n",
        "        dataset_neg.append({\n",
        "            \"race\": str(raw_race),\n",
        "            \"class\": str(class_name_val),\n",
        "            \"level\": int(lvl_val),\n",
        "            \"error\": str(e)\n",
        "        })\n",
        "\n",
        "# SAVE\n",
        "print(f\"INTEGRATION SUMMARY\")\n",
        "print(f\"ACCEPTED: {len(dataset_pos)}\")\n",
        "print(f\"REJECTED: {len(dataset_neg)}\")\n",
        "print(f\"AUGMENTED (Spells Randomized): {augmented_count}\")\n",
        "for reason, count in stats_rejected.most_common():\n",
        "    print(f\"   - {reason:<20} : {count}\")\n",
        "with open(FILE_POS_INT, 'w') as f: json.dump(dataset_pos, f, indent=2)\n",
        "with open(FILE_NEG_INT, 'w') as f: json.dump(dataset_neg, f, indent=2)\n",
        "\n",
        "print(f\"\\nFinal datasets saved to {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKlm7fKNKnoR",
        "outputId": "6eda3904-feb3-4489-8b7f-56a4c987413b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to Google Drive\n",
            "Mounted at /content/drive\n",
            " - Integrating Manual Whitelist: DnD_Integrated.docx\n",
            "   Manual Spells with Levels Loaded: 31\n",
            " - Fetching API Data...\n",
            "Fetching Spell Index (via GraphQL)\n",
            "        Indexed 319 spells via GraphQL.\n",
            "Validation Logic Ready.\n",
            "Validating & Integrating Data from: /content/drive/MyDrive/DnD_Project_Data/dnd_chars_unique.json\n",
            "INTEGRATION SUMMARY\n",
            "ACCEPTED: 2114\n",
            "REJECTED: 5832\n",
            "AUGMENTED (Spells Randomized): 1091\n",
            "   - INVALID_BACKGROUND   : 2898\n",
            "   - INVALID_RACE         : 2048\n",
            "   - INVALID_SUBCLASS     : 817\n",
            "   - UNKNOWN_CLASS        : 45\n",
            "   - MISSING_SUBCLASS     : 14\n",
            "   - INVALID_STATS        : 5\n",
            "   - IMPOSSIBLE_HP        : 5\n",
            "\n",
            "Final datasets saved to /content/drive/MyDrive/DnD_Project_Data/dataset_integrated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Splitting and Formatting\n",
        "\n",
        "Data Loading: Loads the integrated positive and negative datasets generated in the previous step.\n",
        "\n",
        "\n",
        "Task 1: Generation (1/3): Uses the first half of the available positive character sheets\n",
        "\n",
        "Task 2: Completion/Fill (1/3): Uses the second half of the positive character sheets. Fields are masked (set to NULL)\n",
        "\n",
        "Task 3: Refusal (1/3): Uses a subset of negative data\n",
        "\n",
        "\n",
        "Data Cleaning: Applies a specific filter for weapon names (e.g., converting \"Longsword.1\" back to \"Longsword\").\n",
        "\n",
        "Train/Test Split: Performs an 80% Train / 20% Test split within each bucket\n",
        "\n",
        "**Saves two files:**\n",
        "\n",
        "dnd_train.json: The training data formatted for the LLM.\n",
        "\n",
        "dnd_test.json: A separated benchmark file with task types labelled for evaluation."
      ],
      "metadata": {
        "id": "n82i_CackGS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import copy\n",
        "import os\n",
        "import re\n",
        "\n",
        "# PATHS\n",
        "BASE_DIR = \"/content/drive/MyDrive/DnD_Project_Data\"\n",
        "INPUT_DIR = os.path.join(BASE_DIR, \"dataset_integrated\")\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, \"processed_dataset\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "INPUT_POS = os.path.join(INPUT_DIR, \"dataset_integrated_positive.json\")\n",
        "INPUT_NEG = os.path.join(INPUT_DIR, \"dataset_integrated_negative.json\")\n",
        "\n",
        "# Prompt\n",
        "INSTR_GEN = \"Generate a complete D&D 5e character sheet based on the provided attributes.\"\n",
        "INSTR_FILL = \"Complete this D&D 5e character sheet. Replace the NULL value(s) with correct value(s) consistent with the rules.\"\n",
        "GENERIC_REFUSAL_OUTPUT = {\n",
        "    \"message\": \"The provided character data contains invalid, homebrew, or rule-breaking content inconsistent with D&D 5e rules. Cannot generate a character sheet.\"\n",
        "}\n",
        "\n",
        "#  CLEANING FUNCTIONS\n",
        "def clean_vtt_artifacts(char):\n",
        "    \"\"\"\n",
        "    Removes numerical suffixes(e.g., 'Shortsword.1' -> 'Shortsword')\n",
        "    \"\"\"\n",
        "    clean_char = copy.deepcopy(char)\n",
        "    if 'weapons' in clean_char and isinstance(clean_char['weapons'], list):\n",
        "        cleaned_list = []\n",
        "        for w in clean_char['weapons']:\n",
        "            # Regex: Finds a dot followed by numbers at the end of the string\n",
        "            new_w = re.sub(r'\\.\\d+$', '', w)\n",
        "            cleaned_list.append(new_w)\n",
        "        clean_char['weapons'] = cleaned_list\n",
        "    return clean_char\n",
        "\n",
        "def clean_dirty_stats(stats_raw):\n",
        "    \"\"\"Standardizes the visual input format of stats for negative examples.\"\"\"\n",
        "    if not isinstance(stats_raw, dict): return None\n",
        "    clean = {}\n",
        "    for k, v in stats_raw.items():\n",
        "        clean_k = k.lower()[:3]\n",
        "        clean_v = v\n",
        "        if isinstance(v, list): clean_v = v[0] if len(v) > 0 else 0\n",
        "        try: clean_v = int(clean_v)\n",
        "        except: pass\n",
        "        clean[clean_k] = clean_v\n",
        "    return clean\n",
        "\n",
        "def build_input_context(char):\n",
        "    def clean_str(v):\n",
        "        if isinstance(v, (dict, list)): return str(v)\n",
        "        return v\n",
        "    stats_cleaned = clean_dirty_stats(char.get(\"stats\"))\n",
        "    return {\n",
        "        \"race\": clean_str(char.get(\"race\")),\n",
        "        \"subrace\": clean_str(char.get(\"subrace\")),\n",
        "        \"class\": clean_str(char.get(\"class\")),\n",
        "        \"subclass\": clean_str(char.get(\"subclass\")),\n",
        "        \"level\": char.get(\"level\", 1),\n",
        "        \"background\": clean_str(char.get(\"background\")),\n",
        "        \"stats\": stats_cleaned\n",
        "    }\n",
        "\n",
        "def create_masked_entry(char):\n",
        "    \"\"\"\n",
        "    Creates a masked version of the character.\n",
        "    Can mask 1 to 3 fields (including spells, weapons, stats, ...).\n",
        "    \"\"\"\n",
        "    masked = copy.deepcopy(char)\n",
        "\n",
        "    # fields to mask\n",
        "    options = ['alignment', 'subclass', 'background', 'stats', 'spells', 'weapons']\n",
        "\n",
        "    num_to_mask = random.randint(1, 3)\n",
        "\n",
        "    # take n fields random\n",
        "    targets = random.sample(options, num_to_mask)\n",
        "\n",
        "    masked_log_names = []\n",
        "\n",
        "    for target in targets:\n",
        "        # masking for stats\n",
        "        if target == 'stats' and isinstance(masked.get('stats'), dict):\n",
        "            masked['stats'] = {k: None for k in masked['stats']}\n",
        "            masked_log_names.append(\"stats (all)\")\n",
        "        else:\n",
        "            # masking for other\n",
        "            masked[target] = None\n",
        "            masked_log_names.append(target)\n",
        "\n",
        "    return masked, \", \".join(masked_log_names)\n",
        "\n",
        "# LOADING DATA\n",
        "print(\"[Loading Dataset\")\n",
        "try:\n",
        "    with open(INPUT_POS, 'r') as f: data_pos = json.load(f)\n",
        "    with open(INPUT_NEG, 'r') as f: data_neg = json.load(f)\n",
        "    print(f\"    Positive Samples Loaded: {len(data_pos)}\")\n",
        "    print(f\"    Negative Samples Loaded: {len(data_neg)}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Files not found. Please ensure previous steps are completed.\")\n",
        "    raise\n",
        "\n",
        "# BALANCING LOGIC\n",
        "print(\"\\nCalculating Balance (3-Bucket Logic)\")\n",
        "items_pos = data_pos\n",
        "items_neg = data_neg\n",
        "random.shuffle(items_pos)\n",
        "random.shuffle(items_neg)\n",
        "\n",
        "total_pos = len(items_pos)\n",
        "# Divide positives into 2 equal buckets (Half for Gen, Half for Fill)\n",
        "bucket_size = int(total_pos / 2)\n",
        "\n",
        "print(f\"    Positives available: {total_pos}\")\n",
        "print(f\"    Bucket Size calculated (Pos / 2): {bucket_size}\")\n",
        "print(f\"    Negatives needed (equal to one bucket): {bucket_size}\")\n",
        "print(f\"    Negatives discarded: {len(items_neg) - bucket_size}\")\n",
        "\n",
        "# Create Distinct Groups\n",
        "pool_gen = items_pos[:bucket_size]\n",
        "pool_fill = items_pos[bucket_size : bucket_size*2]\n",
        "pool_refusal = items_neg[:bucket_size]\n",
        "\n",
        "# Integrity Check (Ensure no overlap in positive groups)\n",
        "print(f\"    [Check] Intersection verification: Are the first items distinct?\")\n",
        "print(f\"       Gen[0] Race: {pool_gen[0].get('race')} Class: {pool_gen[0].get('class')}\")\n",
        "print(f\"       Fill[0] Race: {pool_fill[0].get('race')} Class: {pool_fill[0].get('class')}\")\n",
        "\n",
        "# TRAIN / TEST SPLIT\n",
        "print(\"\\nSplitting Train (80%) / Test (20%)...\")\n",
        "def split_and_report(name, pool):\n",
        "    split_idx = int(len(pool) * 0.8)\n",
        "    train = pool[:split_idx]\n",
        "    test = pool[split_idx:]\n",
        "    print(f\"    Bucket {name:<10} | Total: {len(pool)} -> Train: {len(train)}, Test: {len(test)}\")\n",
        "    return train, test\n",
        "\n",
        "train_gen, test_gen = split_and_report(\"GEN (Pos)\", pool_gen)\n",
        "train_fill, test_fill = split_and_report(\"FILL (Pos)\", pool_fill)\n",
        "train_ref, test_ref = split_and_report(\"NEG (Ref)\", pool_refusal)\n",
        "\n",
        "# --- 4. DATASET CONSTRUCTION ---\n",
        "print(\"\\nConstructing Final Dataset\")\n",
        "train_dataset = []\n",
        "\n",
        "# Check weapon cleaning\n",
        "dirty_weapon_sample = next((x for x in train_gen if 'weapons' in x and any('.' in w for w in x['weapons'])), None)\n",
        "if dirty_weapon_sample:\n",
        "    print(f\"    [Debug] Weapon Cleaning (Before): {dirty_weapon_sample['weapons']}\")\n",
        "    cleaned_sample = clean_vtt_artifacts(dirty_weapon_sample)\n",
        "    print(f\"    [Debug] Weapon Cleaning (After):  {cleaned_sample['weapons']}\")\n",
        "else:\n",
        "    print(\"No 'dirty' weapon names found in the initial check.\")\n",
        "\n",
        "# Generation Task\n",
        "for p in train_gen:\n",
        "    p_clean = clean_vtt_artifacts(p)\n",
        "    train_dataset.append({\n",
        "        \"instruction\": INSTR_GEN, \"input\": build_input_context(p_clean), \"output\": p_clean\n",
        "    })\n",
        "\n",
        "# Fill Task\n",
        "for p in train_fill:\n",
        "    p_clean = clean_vtt_artifacts(p)\n",
        "    masked_inp, _ = create_masked_entry(p_clean)\n",
        "    train_dataset.append({\n",
        "        \"instruction\": INSTR_FILL, \"input\": masked_inp, \"output\": p_clean\n",
        "    })\n",
        "\n",
        "# Refusal Task\n",
        "for n in train_ref:\n",
        "    train_dataset.append({\n",
        "        \"instruction\": INSTR_GEN, \"input\": build_input_context(n), \"output\": GENERIC_REFUSAL_OUTPUT\n",
        "    })\n",
        "\n",
        "random.shuffle(train_dataset)\n",
        "print(f\"    Train Set Constructed: {len(train_dataset)} total samples.\")\n",
        "\n",
        "# Test set\n",
        "print(\"\\n Constructing Test Set\")\n",
        "test_benchmark = []\n",
        "\n",
        "for p in test_gen:\n",
        "    p_clean = clean_vtt_artifacts(p)\n",
        "    clean_inp = build_input_context(p_clean)\n",
        "    test_benchmark.append({\n",
        "        \"task_type\": \"generation\",\n",
        "        \"llm_prompt\": f\"{INSTR_GEN}\\n\\nInput Data:\\n{json.dumps(clean_inp)}\",\n",
        "        \"expected_output\": p_clean\n",
        "    })\n",
        "\n",
        "for p in test_fill:\n",
        "    p_clean = clean_vtt_artifacts(p)\n",
        "    masked_inp, fname = create_masked_entry(p_clean)\n",
        "    test_benchmark.append({\n",
        "        \"task_type\": \"completion\",\n",
        "        \"masked_field\": fname,\n",
        "        \"llm_prompt\": f\"{INSTR_FILL}\\n\\nCharacter Sheet:\\n{json.dumps(masked_inp)}\",\n",
        "        \"expected_output\": p_clean\n",
        "    })\n",
        "\n",
        "for n in test_ref:\n",
        "    unified_inp = build_input_context(n)\n",
        "    test_benchmark.append({\n",
        "        \"task_type\": \"refusal\",\n",
        "        \"llm_prompt\": f\"{INSTR_GEN}\\n\\nInput Data:\\n{json.dumps(unified_inp)}\",\n",
        "        \"expected_output\": GENERIC_REFUSAL_OUTPUT\n",
        "    })\n",
        "\n",
        "print(f\"Test set constructed: {len(test_benchmark)} total samples.\")\n",
        "\n",
        "# SAVE\n",
        "print(\"Saving\")\n",
        "with open(os.path.join(OUTPUT_DIR, \"dnd_train.json\"), 'w') as f:\n",
        "    json.dump(train_dataset, f, indent=2)\n",
        "with open(os.path.join(OUTPUT_DIR, \"dnd_test.json\"), 'w') as f:\n",
        "    json.dump(test_benchmark, f, indent=2)\n",
        "\n",
        "print(f\"Files saved in: {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VxpDSUmX9QK",
        "outputId": "aeb9f9fc-3a2d-4b68-9789-d32caef15bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/6] Loading Dataset...\n",
            "    Positive Samples Loaded: 2114\n",
            "    Negative Samples Loaded: 5832\n",
            "\n",
            "[2/6] Calculating Balance (3-Bucket Logic)...\n",
            "    Positives available: 2114\n",
            "    Bucket Size calculated (Pos / 2): 1057\n",
            "    Negatives needed (equal to one bucket): 1057\n",
            "    Negatives discarded: 4775\n",
            "    [Check] Intersection verification: Are the first items distinct?\n",
            "       Gen[0] Race: Orc Class: Fighter\n",
            "       Fill[0] Race: Human Class: Rogue\n",
            "\n",
            "[3/6] Splitting Train (80%) / Test (20%)...\n",
            "    Bucket GEN (Pos)  | Total: 1057 -> Train: 845, Test: 212\n",
            "    Bucket FILL (Pos) | Total: 1057 -> Train: 845, Test: 212\n",
            "    Bucket NEG (Ref)  | Total: 1057 -> Train: 845, Test: 212\n",
            "\n",
            "[4/6] Constructing Final Dataset & Cleaning...\n",
            "    [Debug] Weapon Cleaning (Before): ['Handaxe', 'Handaxe.1', 'Longbow']\n",
            "    [Debug] Weapon Cleaning (After):  ['Handaxe', 'Handaxe', 'Longbow']\n",
            "    Train Set Constructed: 2535 total samples.\n",
            "\n",
            "[5/6] Constructing Benchmark (Test Set)...\n",
            "    Test Benchmark Constructed: 636 total samples.\n",
            "\n",
            "[6/6] Saving to Disk...\n",
            "    Files saved in: /content/drive/MyDrive/DnD_Project_Data/processed_dataset\n"
          ]
        }
      ]
    }
  ]
}